{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from circuit_breaking.src import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import contextlib\n",
    "import einops\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "from collections import defaultdict\n",
    "dotenv.load_dotenv()\n",
    "import math\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "\n",
    "HF_ACCESS_TOKEN = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "if HF_ACCESS_TOKEN:\n",
    "    login(token=HF_ACCESS_TOKEN)\n",
    "    print(\"Successfully authenticated with Hugging Face.\")\n",
    "else:\n",
    "    print(\"Hugging Face access token not found in environment variables.\")\n",
    "\n",
    "if WANDB_API_KEY:\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "    print(\"Successfully authenticated with Weights & Biases.\")\n",
    "else:\n",
    "    print(\"Weights & Biases API key not found in environment variables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"google/gemma-2-9b\"\n",
    "model_type = \"gemma-2\"\n",
    "other_model_type = \"gemma2_9b\"\n",
    "# pretrained_path = \"/data/huggingface/models--google--gemma-2-9b/snapshots/33c193028431c2fde6c6e51f29e6f17b60cbfac6/\"\n",
    "# pretrained_path = \"/data/huggingface/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/\"\n",
    "pretrained_path = None\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "left_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "left_tokenizer.pad_token_id = left_tokenizer.eos_token_id\n",
    "left_tokenizer.padding_side = \"left\"\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "if pretrained_path is not None:\n",
    "    model = AutoModelForCausalLM.from_pretrained(pretrained_path, torch_dtype=dtype)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype)\n",
    "model.cuda()\n",
    "n_layers = 42\n",
    "n_heads = 16\n",
    "n_kv_heads = 8\n",
    "\n",
    "param_count_dict = {\"attn.hook_q\": 3584*4096, \"attn.hook_k\": 3584*2048, \"attn.hook_v\": 3584*2048, \"attn.hook_result\": 4096*3584, \"mlp.hook_pre\": 3584 * 14336, \"mlp.hook_post\": 14336 * 3584, \"mlp.hook_gate\": 3584 * 14336}\n",
    "mmlu_batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "sae_type = \"16k\"\n",
    "# sae_layer = 21\n",
    "\n",
    "sae_dict = {}\n",
    "\n",
    "sae_layers = list(range(42))\n",
    "# 3.76 gb + .47 gb = 4.23 gb\n",
    "for layer in tqdm(sae_layers):\n",
    "    if sae_type == \"131k\":\n",
    "        sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "            release = \"gemma-scope-9b-pt-res-canonical\", # e.g., \"gpt2-small-res-jb\". See other options in https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n",
    "            sae_id = f\"layer_{layer}/width_131k/canonical\", # e.g., \"blocks.8.hook_resid_pre\". Won't always be a hook point\n",
    "        )\n",
    "    elif sae_type == \"16k\":\n",
    "        sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "            release = \"gemma-scope-9b-pt-res-canonical\", # e.g., \"gpt2-small-res-jb\". See other options in https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n",
    "            sae_id = f\"layer_{layer}/width_16k/canonical\", # e.g., \"blocks.8.hook_resid_pre\". Won't always be a hook point\n",
    "        )\n",
    "    \n",
    "    # for param in sae.parameters():\n",
    "    #     param.data = param.data.to(torch.bfloat16)\n",
    "\n",
    "    sae_dict[layer] = sae.cuda()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
