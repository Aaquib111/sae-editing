{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/sae-editing\n",
      "Successfully authenticated with Hugging Face.\n",
      "Successfully authenticated with Weights & Biases.\n"
     ]
    }
   ],
   "source": [
    "# from circuit_breaking.src import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import contextlib\n",
    "import einops\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "from collections import defaultdict\n",
    "dotenv.load_dotenv()\n",
    "import json\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "HF_ACCESS_TOKEN = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "if HF_ACCESS_TOKEN:\n",
    "    login(token=HF_ACCESS_TOKEN)\n",
    "    print(\"Successfully authenticated with Hugging Face.\")\n",
    "else:\n",
    "    print(\"Hugging Face access token not found in environment variables.\")\n",
    "\n",
    "if WANDB_API_KEY:\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "    print(\"Successfully authenticated with Weights & Biases.\")\n",
    "else:\n",
    "    print(\"Weights & Biases API key not found in environment variables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype = torch.bfloat16\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\", torch_dtype=dtype)\n",
    "# del model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"PhillipGuo/gemma-2-sae-masked-gd-mc-6-fullrank\", torch_dtype=dtype)\n",
    "# del model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"PhillipGuo/gemma-2-sae-gd-fullrank\", torch_dtype=dtype)\n",
    "# del model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"PhillipGuo/gemma-2-gd-mc-5-fullrank\", torch_dtype=dtype)\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838e8eed37534f639ea2ddb5cf39769a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853bad290acd4722bcdba331b7ab6bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cc226a90c443838ad50fc79ca4a9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b60dd776b5b4db09d98866e51c7742d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24128934a9864dabbc6508a066661ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = \"google/gemma-2-9b\"\n",
    "model_type = \"gemma-2\"\n",
    "other_model_type = \"gemma2_9b\"\n",
    "# pretrained_path = \"/data/huggingface/models--google--gemma-2-9b/snapshots/33c193028431c2fde6c6e51f29e6f17b60cbfac6/\"\n",
    "# pretrained_path = \"/data/huggingface/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/\"\n",
    "pretrained_path = None\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "left_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "left_tokenizer.pad_token_id = left_tokenizer.eos_token_id\n",
    "left_tokenizer.padding_side = \"left\"\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "if pretrained_path is not None:\n",
    "    model = AutoModelForCausalLM.from_pretrained(pretrained_path, torch_dtype=dtype)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype)\n",
    "# model.cuda()\n",
    "n_layers = 42\n",
    "n_heads = 16\n",
    "n_kv_heads = 8\n",
    "\n",
    "param_count_dict = {\"attn.hook_q\": 3584*4096, \"attn.hook_k\": 3584*2048, \"attn.hook_v\": 3584*2048, \"attn.hook_result\": 4096*3584, \"mlp.hook_pre\": 3584 * 14336, \"mlp.hook_post\": 14336 * 3584, \"mlp.hook_gate\": 3584 * 14336}\n",
    "mmlu_batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biology loss: 2.4375\n",
      "Biology accuracy: 69.8%\n",
      "Biology loss (injection): 3.34375\n",
      "Biology accuracy (injection): 2.9%\n",
      "MMLU accuracy: 73%\n"
     ]
    }
   ],
   "source": [
    "from tasks.wmdp.WMDP_MCTask import WMDP_MCTask\n",
    "from tasks.wmdp.WMDP_RelearnTask import WMDP_RelearnTask\n",
    "from tasks.wmdp.WMDP_UnlearnTask import WMDP_UnlearnTask, WMDP_UnlearnMCTask\n",
    "from tasks.general_capabilities.MCTask_redo import run_general_evals\n",
    "batch_size = 8\n",
    "# bio_task = WMDP_UnlearnTask(batch_size=batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True, split=\"first_two\", train_test_split=False, criterion=\"cross_entropy\", injection_task=False)\n",
    "# bio_mc_task = WMDP_UnlearnMCTask(batch_size=batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True, split=\"first_two\", train_test_split=False, injection_task=False)\n",
    "\n",
    "# injection_bio_task = WMDP_UnlearnTask(batch_size=batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True, split=\"first_two\", train_test_split=False, criterion=\"cross_entropy\", injection_task=True)\n",
    "# injection_bio_mc_task = WMDP_UnlearnMCTask(batch_size=batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True, split=\"first_two\", train_test_split=False, injection_task=True)\n",
    "# print(bio_mc_task.test_dataset)\n",
    "# # acc = 0\n",
    "# num_iters = len(bio_mc_task.test_dataset) // batch_size\n",
    "\n",
    "\n",
    "# loss = bio_task.get_test_loss(model, n_iters=num_iters)\n",
    "# print(f\"Biology loss: {loss}\")\n",
    "# acc = bio_mc_task.get_test_accuracy(model, n_iters=num_iters, continuous=False)\n",
    "# print(f\"Biology accuracy: {acc}\")\n",
    "# loss = injection_bio_task.get_test_loss(model, n_iters=num_iters)\n",
    "# print(f\"Biology loss (injection): {loss}\")\n",
    "# acc = injection_bio_mc_task.get_test_accuracy(model, n_iters=num_iters, continuous=False)\n",
    "# print(f\"Biology accuracy (injection): {acc}\")\n",
    "\n",
    "# mmlu = run_general_evals(model, model_type=model_type, evals_to_include=[\"MMLU\"], verbose=True, batch_size=5, device=\"cuda\")\n",
    "# print(f\"MMLU accuracy: {mmlu['MMLU']}\")\n",
    "\n",
    "\n",
    "print(\"Biology loss: 2.4375\")\n",
    "print(\"Biology accuracy: 69.8%\")\n",
    "print(\"Biology loss (injection): 3.34375\")\n",
    "print(\"Biology accuracy (injection): 2.9%\")\n",
    "print(\"MMLU accuracy: 73%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tasks.wmdp.WMDP_MCTask import WMDP_DedupedTask\n",
    "# relearn_bio_mc_task = WMDP_DedupedTask(batch_size=batch_size, tokenizer=tokenizer, subset=\"wmdp-bio-retrain\", shuffle=True)\n",
    "# print(relearn_bio_mc_task.get_test_accuracy(model, use_test_data=False, num_iters=10, continuous=False))\n",
    "# print(relearn_bio_mc_task.get_test_accuracy(model, num_iters=10, continuous=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'abstract', 'text', 'doi'],\n",
       "    num_rows: 24453\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2940fb3c1dfe49dc9713bc31f5a05b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108d7bb7ede44fe68bb7a6d0ac772356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test dataset available. Using train dataset for testing.\n",
      "{'text': ['It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web. Playing on the web works, but you have to simulate multi-touch for table moving and that can be a bit confusing.\\n\\nThere’s a lot I’d like to talk about. I’ll go through every topic, insted of making the typical what went right/wrong list.\\n\\nConcept\\n\\nWorking over the theme was probably one of the hardest tasks I had to face.\\n\\nOriginally, I had an idea of what kind of game I wanted to develop, gameplay wise – something with lots of enemies/actors, simple graphics, maybe set in space, controlled from a top-down view. I was confident I could fit any theme around it.\\n\\nIn the end, the problem with a theme like “Evolution” in a game is that evolution is unassisted. It happens through several seemingly random mutations over time, with the most apt permutation surviving. This genetic car simulator is, in my opinion, a great example of actual evolution of a species facing a challenge. But is it a game?\\n\\nIn a game, you need to control something to reach an objective. That control goes against what evolution is supposed to be like. If you allow the user to pick how to evolve something, it’s not evolution anymore – it’s the equivalent of intelligent design, the fable invented by creationists to combat the very idea of evolution. Being agnostic and a Pastafarian, that’s not something that rubbed me the right way.\\n\\nHence, my biggest dillema when deciding what to create was not with what I wanted to create, but with what I did not. I didn’t want to create an “intelligent design” simulator and wrongly call it evolution.\\n\\nThis is a problem, of course, every other contestant also had to face. And judging by the entries submitted, not many managed to work around it. I’d say the only real solution was through the use of artificial selection, somehow. So far, I haven’t seen any entry using this at its core gameplay.\\n\\nAlas, this is just a fun competition and after a while I decided not to be as strict with the game idea, and allowed myself to pick whatever I thought would work out.\\n\\nMy initial idea was to create something where humanity tried to evolve to a next level but had some kind of foe trying to stop them from doing so. I kind of had this image of human souls flying in space towards a monolith or a space baby (all based in 2001: A Space Odyssey of course) but I couldn’t think of compelling (read: serious) mechanics for that.\\n\\nBorgs were my next inspiration, as their whole hypothesis fit pretty well into the evolution theme. But how to make it work? Are you the borg, or fighting the Borg?\\n\\nThe third and final idea came to me through my girlfriend, who somehow gave me the idea of making something about the evolution of Pasta. The more I thought about it the more it sounded like it would work, so I decided to go with it.\\n\\nConversations with my inspiring co-worker Roushey (who also created the “Mechanical Underdogs” signature logo for my intros) further matured the concept, as it involved into the idea of having individual pieces of pasta flying around and trying to evolve until they became all-powerful. A secondary idea here was that the game would work to explain how the Flying Spaghetti Monster came to exist – by evolving from a normal dinner table.\\n\\nSo the idea evolved more or less into this: you are sitting a table. You have your own plate, with is your “base”. There are 5 other guests at the table, each with their own plate.\\n\\nYour plate can spawn little pieces of pasta. You do so by “ordering” them through a menu. Some pastas are better than others; some are faster, some are stronger. They have varying costs, which are debited from your credits (you start with a number of credits).\\n\\nOnce spawned, your pastas start flying around. Their instinct is to fly to other plates, in order to conquer them (the objective of the game is having your pasta conquer all the plates on the table). But they are really autonomous, so after being spawned, you have no control over your pasta (think DotA or LoL creeps).\\n\\nYour pasta doesn’t like other people’s pasta, so if they meet, they shoot sauce at each other until one dies. You get credits for other pastas your own pasta kill.\\n\\nOnce a pasta is in the vicinity of a plate, it starts conquering it for its team. It takes around 10 seconds for a plate to be conquered; less if more pasta from the same team are around. If pasta from other team are around, though, they get locked down in their attempt, unable to conquer the plate, until one of them die (think Battlefield’s standard “Conquest” mode).\\n\\nYou get points every second for every plate you own.\\n\\nOver time, the concept also evolved to use an Italian bistro as its main scenario.\\n\\nCarlos, Carlos’ Bistro’s founder and owner\\n\\nSetup\\n\\nNo major changes were made from my work setup. I used FDT and Starling creating an Adobe AIR (ActionScript) project, all tools or frameworks I already had some knowledge with.\\n\\nOne big change for me was that I livestreamed my work through a twitch.tv account. This was a new thing for me. As recommended by Roushey, I used a program called XSplit and I got to say, it is pretty amazing. It made the livestream pretty effortless and the features are awesome, even for the free version. It was great to have some of my friends watch me, and then interact with them and random people through chat. It was also good knowing that I was also recording a local version of the files, so I could make a timelapse video later.\\n\\nKnowing the video was being recorded also made me a lot more self-conscious about my computer use, as if someone was watching over my shoulder. It made me realize that sometimes I spend too much time in seemingly inane tasks (I ended up wasting the longest time just to get some text alignment the way I wanted – it’ll probably drive someone crazy if they watch it) and that I do way too many typos where writing code. I pretty much spend half of the time writing a line and the other half fixing the crazy characters in it.\\n\\nMy own stream was probably boring to watch since I was coding for the most time. But livestreaming is one of the cool things to do as a spectator too. It was great seeing other people working – I had a few tabs opened on my second monitor all the time. It’s actually a bit sad, because if I could, I could have spent the whole weekend just watching other people working! But I had to do my own work, so I’d only do it once in a while, when resting for a bit.\\n\\nDesign\\n\\nAlthough I wanted some simple, low-fi, high-contrast kind of design, I ended up going with somewhat realistic (vector) art. I think it worked very well, fitting the mood of the game, but I also went overboard.\\n\\nFor example: to know the state of a plate (who owns it, who’s conquering it and how much time they have left before conquering it, which pasta units are in the queue, etc), you have to look at the plate’s bill.\\n\\nThe problem I realized when doing some tests is that people never look at the bill! They think it’s some kind of prop, so they never actually read its details.\\n\\nPlus, if you’re zoomed out too much, you can’t actually read it, so it’s hard to know what’s going on with the game until you zoom in to the area of a specific plate.\\n\\nOne other solution that didn’t turn out to be as perfect as I thought was how to indicate who a plate base belongs to. In the game, that’s indicated by the plate’s decoration – its color denotes the team owner. But it’s something that fits so well into the design that people never realized it, until they were told about it.\\n\\nIn the end, the idea of going with a full physical metaphor is one that should be done with care. Things that are very important risk becoming background noise, unless the player knows its importance.\\n\\nOriginally, I wanted to avoid any kind of heads-up display in my game. In the end, I ended up adding it at the bottom to indicate your credits and bases owned, as well as the hideous out-of-place-and-still-not-obvious “Call Waiter” button. But in hindsight, I should have gone with a simple HUD from the start, especially one that indicated each team’s colors and general state of the game without the need for zooming in and out.\\n\\nDevelopment\\n\\nDevelopment went fast. But not fast enough.\\n\\nEven though I worked around 32+ hours for this Ludum Dare, the biggest problem I had to face in the end was overscoping. I had too much planned, and couldn’t get it all done.\\n\\nContent-wise, I had several kinds of pasta planned (Wikipedia is just amazing in that regard), split into several different groups, from small Pastina to huge Pasta al forno. But because of time constraints, I ended up scratching most of them, and ended up with 5 different types of very small pasta – barely something to start when talking about the evolution of Pasta.\\n\\nPastas used in the game. Unfortunately, the macs where never used\\n\\nWhich is one of the saddest things about the project, really. It had the framework and the features to allow an endless number of elements in there, but I just didn’t have time to draw the rest of the assets needed (something I loved to do, by the way).\\n\\nOther non-obvious features had to be dropped, too. For example, when ordering some pasta, you were supposed to select what kind of sauce you’d like with your pasta, each with different attributes. Bolognese, for example, is very strong, but inaccurate; Pesto is very accurate and has great range, but it’s weaker; and my favorite, Vodka, would triggers 10% loss of speed on the pasta hit by it.\\n\\nThe code for that is mostly in there. But in the end, I didn’t have time to implement the sauce selection interface; all pasta ended up using bolognese sauce.\\n\\nTo-do list: lots of things were not done\\n\\nActual programming also took a toll in the development time. Having been programming for a while, I like to believe I got to a point where I know how to make things right, but at the expense of forgetting how to do things wrong in a seemingly good way. What I mean is that I had to take a lot of shortcuts in my code to save time (e.g. a lot of singletons references for cross-communication rather than events or observers, all-encompassing check loops, not fast enough) that left a very sour taste in my mouth. While I know I used to do those a few years ago and survive, I almost cannot accept the state my code is in right now.\\n\\nAt the same time, I do know it was the right thing to do given the timeframe.\\n\\nOne small thing that had some impact was using a somewhat new platform for me. That’s Starling, the accelerated graphics framework I used in Flash. I had tested it before and I knew how to use it well – the API is very similar to Flash itself. However, there were some small details that had some impact during development, making me feel somewhat uneasy the whole time I was writing the game. It was, again, the right thing to do, but I should have used Starling more deeply before (which is the conundrum: I used it for Ludum Dare just so I could learn more about it).\\n\\nArgument and user experience\\n\\nOne final aspect of the game that I learned is that making the game obvious for your players goes a long way into making it fun. If you have to spend the longest time explaining things, your game is doing something wrong.\\n\\nAnd that’s exactly the problem Survival of the Tastiest ultimately faced. It’s very hard for people to understand what’s going on with the game, why, and how. I did have some introductory text at the beginning, but that was a last-minute thing. More importantly, I should have had a better interface or simplified the whole concept so it would be easier for people to understand.\\n\\nThat doesn’t mean the game itself should be simple. It just means that the experience and interface should be approachable and understandable.\\n\\nConclusion\\n\\nI’m extremely happy with what I’ve done and, especially given that this was my first Ludum Dare. However, I feel like I’ve learned a lot of what not to do.\\n\\nThe biggest problem is overscoping. Like Eric Decker said, the biggest lesson we can learn with this is probably with scoping – deciding what to do beforehand in a way you can complete it without having to rush and do something half-assed.\\n\\nI’m sure I will do more Ludum Dares in the future. But if there are any lessons I can take of it, they are to make it simple, to use frameworks and platforms you already have some absolute experience with (otherwise you’ll spend too much time trying to solve easy questions), and to scope for a game that you can complete in one day only (that way, you can actually take two days and make it cool).\\n\\nThis entry was posted\\non Monday, August 27th, 2012 at 10:54 am and is filed under LD #24.\\nYou can follow any responses to this entry through the RSS 2.0 feed.\\nYou can skip to the end and leave a response. Pinging is currently not allowed.\\n\\n3 Responses to ““Survival of the Tastiest” Post-mortem”\\n\\ndarn it , knowing that I missed your livestream makes me a sad panda ;( but more to the point, the game is … well for a startup its original to say the least ;D it has some really neat ideas and more importantly its designed arround touch screens whitch by the looks of the submission is something rare ;o or that could be just me and my short memory -_-! awesum game, love et <3', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\r\\n<segment>\\r\\n    <name>PD1</name>\\r\\n    <description>Patient Additional Demographic</description>\\r\\n    <elements>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.1</name>\\r\\n            <description>Living Dependency</description>\\r\\n            <datatype>IS</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.2</name>\\r\\n            <description>Living Arrangement</description>\\r\\n            <datatype>IS</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.3</name>\\r\\n            <description>Patient Primary Facility</description>\\r\\n            <datatype>XON</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.4</name>\\r\\n            <description>Patient Primary Care Provider Name &amp; ID No.</description>\\r\\n            <datatype>XCN</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.5</name>\\r\\n            <description>Student Indicator</description>\\r\\n            <datatype>IS</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.6</name>\\r\\n            <description>Handicap</description>\\r\\n            <datatype>IS</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.7</name>\\r\\n            <description>Living Will Code</description>\\r\\n            <datatype>IS</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.8</name>\\r\\n            <description>Organ Donor Code</description>\\r\\n            <datatype>IS</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.9</name>\\r\\n            <description>Separate Bill</description>\\r\\n            <datatype>ID</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.10</name>\\r\\n            <description>Duplicate Patient</description>\\r\\n            <datatype>CX</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.11</name>\\r\\n            <description>Publicity Code</description>\\r\\n            <datatype>CE</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.12</name>\\r\\n            <description>Protection Indicator</description>\\r\\n            <datatype>ID</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.13</name>\\r\\n            <description>Protection Indicator Effective Date</description>\\r\\n            <datatype>DT</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.14</name>\\r\\n            <description>Place of Worship</description>\\r\\n            <datatype>XON</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.15</name>\\r\\n            <description>Advance Directive Code</description>\\r\\n            <datatype>CE</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.16</name>\\r\\n            <description>Immunization Registry Status</description>\\r\\n            <datatype>IS</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.17</name>\\r\\n            <description>Immunization Registry Status Effective Date</description>\\r\\n            <datatype>DT</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.18</name>\\r\\n            <description>Publicity Code Effective Date</description>\\r\\n            <datatype>DT</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.19</name>\\r\\n            <description>Military Branch</description>\\r\\n            <datatype>IS</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.20</name>\\r\\n            <description>Military Rank/Grade</description>\\r\\n            <datatype>IS</datatype>\\r\\n        </field>\\r\\n        <field minOccurs=\"0\" maxOccurs=\"0\">\\r\\n            <name>PD1.21</name>\\r\\n            <description>Military Status</description>\\r\\n            <datatype>IS</datatype>\\r\\n        </field>\\r\\n    </elements>\\r\\n</segment>\\r\\n', 'Topic: reinvent midnight madness\\n\\nAmazon announced a new service at the AWS re:Invent Midnight Madness event. Amazon Sumerian is a solution that aims to make it easier for developers to build virtual reality, augmented reality, and 3D applications. It features a user friendly editor, which can be used to drag and drop 3D objects and characters into scenes. Amazon … continue reading', 'About Grand Slam Fishing Charters\\n\\nAs a family owned business we know how important it is that your trip becomes the best memory of your vacation, we are proud of our islands, our waters and our crew and we are desperate show you the best possible time during your stay. We can not guarantee fish every time but we can guarantee you a great time! The biggest perk of our job is seeing so many of our customers become close friends”\\n\\nA Great Way To Make New Friends!\\n\\nOur dockside parties are a great way to make new friends! Everyone is welcome!\\n\\nAndrea runs the whole operation, from discussing your initial needs by phone or email through to ensuring you have sufficient potato chips. Andrea has worked as concierge for many International resorts and fully understands the high expectations of international visitors.\\n\\n“Life’s A Game But Fishing Is Serious!”\\n\\nUnlike many tour operators, our crew are highly valued and have been with us since day 1. Each have their own personalities and sense of humour and understand the importance of making your day perfect, for us the saying is true, “Lifes a game but fishing is serious!”\\n\\nTRIP ADVISOR\\n\\nPlan Your Trip!\\n\\nAJ and Earl were excellent. My son and I did a half day deep sea trip and though the fish weren’t too cooperative, they did everything to try to get something to bite. Very knowledgeable about the waters and my son was able to land a nice barracuda. The next day my wife, daughter, son […]\\n\\nWhen we arrived the crew made us feel right at home. They made us feel comfortable and answered all questions. The crew worked hard all day to put us on fish. We were successful in landing a nice size Wahoo even though the weather did not cooperate the entire day was enjoyable. I highly recommend […]', \"Q:\\n\\nWhy was Mundungus banned from the Hog's Head?\\n\\nIn Order of the Phoenix while the trio were in the Hogs Head for the first time plotting the start of Dumbledore's Army, it transpires that ol' Dung was lurking in the pub in a disguise, having been banned 20 years previously according to Sirius. \\nFirstly, why was he banned? this could possibly be the tight spot that Albus had helped Dung with in the first place that made him loyal to Albus.  \\nAnd secondly, how is it that he is then speaking to Aberforth in Halfblood Prince? (assuming the ban was for something rather unforgivable, 20 years is a long time?) \\nThey both could have been in the Order by then, but unlikely given Aberforth's attitude in Deathly Hallows once the trio arrive in Hogsmeade looking for the tiara.  We learn now that a lot of trafficking goes on through the Hogs Head so maybe Dung was trading with Aberforth, Sirius' mirror and various other Black artifacts, he just was not allowed in the pub. \\nAnyone with something in canon or more plausible?\\n\\nA:\\n\\nwhy was he banned?\\nI'm not able to find any canon data on that, either book text search or interviews transcripts.\\n\\nhow is it that he is then speaking to Aberforth in Halfblood Prince?\\nIn HBP, he's speaking to Aberforth, NOT being inside Hog's Head. The topic was selling stuff he stole from Sirius' place:\\n\\nNikki: How did sirius twoway mirror end up with aberforth or is it another twoway mirror?\\n  J.K. Rowling: You see Aberforth meeting Mundungus in Hogsmeade. That was the occasion on which Dung, who had taken Sirius’s mirror from Grimmauld Place, sold it to Aberforth.\\n  (src: J.K. Rowling Interview / The Deathly Hallows Web Chat / July 2007)\\n\\nAs a note - this was important since one of the things sold was the 2-way mirror that Harry used to request help when they were imprisoned at Malfoy's in DH.\\nSo, he was banned from the pub (probably, to avoid causing Aberforth's establishment further trouble), but doesn't mean Aberforth won't talk/do business with him otherwise.\\n\\n\", 'Working Women, Special Provision and the Debate on Equality\\n\\nThere has been considerable coverage in the media recently about the possibility of offering women in employment paid leave from work during their menstrual period. This has generated a broad range of responses relating to long-standing discussions about ‘equality’ and ‘difference’: is women’s equality best achieved by treating them the same as men or by making provisions that recognise their differences in terms of physiological constitution and biological functions?\\n\\nIf the UK introduces such an initiative, it would not be the first country in the contemporary world to do so. Many countries in Asia already make the provision and Russia debated introducing a law in 2013. The policy also has a significant historical precedent. A whole chapter of my book Women Workers in the Soviet Interwar Economy: From ‘Protection’ to ‘Equality’ (Macmillan, 1999), based on extensive research conducted for my PhD, is devoted to ‘Provision for “Menstrual Leave”’.\\n\\nIn the 1920s, scientific researchers and labour hygiene specialists in the Soviet Union conducted extensive investigations into the impact of menstruation on women’s capacity to work in manual and industrial jobs requiring a significant degree of physical labour. Their recommendations led to two decrees being issued that targeted specific categories of women workers:\\n\\nDecree ‘On the release from work during menstruation of machinists and iron press workers working on cutting machines without mechanised gears in the garment industry’, 11 January 1922\\n\\nDecree ‘On the working conditions of women tractor and lorry drivers’, 9 May 1931\\n\\nThese decrees arose from research that suggested, amongst other things, that inadequate seating at machines and on tractors resulted in congestion and tension in the abdomen that was exacerbated during menstruation. In practice, the decrees did not provide for regular absence from work. Women seeking to benefit from the provision had to provide a doctor’s note, similar to the usual requirements for sick leave.\\n\\nThe official research into the impact of menstruation on women’s capacity to work and the application of the decrees in practice raised a number of issues on both sides of the argument. I offer only a summary of the contemporary research findings and observer commentary here:\\n\\nFor the provision:\\n• employers have a responsibility to protect the health of their workers and unhealthy, poor and inadequate working environments can have a detrimental impact on women’s reproductive health\\n• women’s labour productivity and output would rise as a result\\n• it is essential to protect the professionalism of certain categories of workers: the debates here centred on performance artists and female theatrical employees engaged in highly physical and intensely emotional work\\n• heavy physical labour and strenuous exercise can lead to disruptions of the menstrual cycle\\n• women’s physical and intellectual capacities are reduced during menstruation; women lose muscular strength and powers of concentration\\n• women’s biological constitution and reproductive functions require specific recognition in law\\n\\nAgainst the provision:\\n• employers are less likely to appoint women if they are guaranteed paid time off work during menstruation\\n• (often from male workers, who viewed the employment of women as competition) women should not be employed in jobs for which they lack the physical strength and mental capacity\\n• if necessary, women could be transferred to different tasks involving easier work during menstruation\\n• the provision would be open to uneven application and abuse\\n• women cannot expect to be considered equal with men if they are given special treatment in the law\\n\\nIt is worth noting also that the various research projects often revealed that the vast majority of women reported no regular problems or abnormalities with menstruation, and that men commonly reported higher levels of sickness than their female colleagues. Many of the problems experienced by women in the workplace could be mitigated by the introduction of improvements to their physical working conditions (not sitting down or standing up in the same position for long periods of time) or by the simple introduction of very short breaks that would allow women to walk around and get some exercise.\\n\\nDebates in the UK, on the TV and in the press, are unlikely to reach a consensus on this issue. What do you think?', \"Q:\\n\\nUsing M-Test to show you can differentiate term by term.\\n\\nI have the series $\\\\sum_{n=1}^\\\\infty \\\\frac{\\\\lambda^{n-1}n}{n!}=\\\\sum_{n=1}^\\\\infty \\\\frac{d}{d\\\\lambda}\\\\big(\\\\frac{\\\\lambda^n}{n!} \\\\big)$\\nand I would like it to be $\\\\frac{d}{d\\\\lambda}\\\\big(\\\\sum_{n=1}^\\\\infty \\\\frac{\\\\lambda^n}{n!})$.\\nI'm trying to show that this sequence of functions converges uniformly on $(0,\\\\infty)$ and so I'm trying the M-Test.  So I need to find bounds $M_n$ for $\\\\big|\\\\frac{\\\\lambda^n}{n!}\\\\big|$, such that $\\\\sum M_n$ converges.\\nThanks.  This is in order to show that I can actually do the differentiation term by term.\\n\\nA:\\n\\nYou deal with a power series with radius of convergence $R=+\\\\infty$ so you can differentiate term by term.\\n\\n\", 'Jeanette Sawyer Cohen, PhD, clinical assistant professor of psychology in pediatrics at Weill Cornell Medical College in New York City\\n\\nPediatric Psychologist\\n\\nHow to Teach Independence?\\n\\nHow can I teach my toddler to do things independently?\\n\\nYou’ve probably become more patient since you started this whole parenthood thing. And you’re going to have to practice patience even more as your toddler learns to become more independent.\\n\\nFor example, she tells you she can’t finish the puzzle she’s doing. Instead of jumping right in and telling her which piece goes where, you’re going to have to tell her you’ll help a little. Go ahead and help, but let her do a lot of it herself, and make sure she’s the one to finish the job. That will give her a sense of accomplishment and the confidence to try again next time.\\n\\nRemember that children each progress at their own rate. It’s not always fast — and there will be setbacks along the way. But the more you can allow them to do on their own without stepping in, the more they’ll be likely to try for themselves again and again.'], 'meta': {'pile_set_name': ['Pile-CC', 'Github', 'Pile-CC', 'Pile-CC', 'StackExchange', 'Pile-CC', 'StackExchange', 'Pile-CC']}}\n"
     ]
    }
   ],
   "source": [
    "bio_ce_task = WMDP_RelearnTask(batch_size=batch_size, tokenizer=tokenizer, corpus=\"bio-forget\", shuffle=True)\n",
    "display(bio_ce_task.dataset)\n",
    "\n",
    "from tasks.general.DatasetTasks import PileTask\n",
    "pile_task = PileTask(batch_size=batch_size, tokenizer=tokenizer, stream_dataset=True, buffer_size=10000, ctx_length=500)\n",
    "print(pile_task.get_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a5b923c9fb4afca0fd9c33bd2fb880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.380687713623047 GB allocated\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "sae_type = \"16k\"\n",
    "# sae_layer = 21\n",
    "\n",
    "sae_dict = {}\n",
    "\n",
    "sae_layers = range(n_layers)\n",
    "# 3.76 gb + .47 gb = 4.23 gb\n",
    "for layer in tqdm(sae_layers):\n",
    "    if sae_type == \"131k\":\n",
    "        sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "            release = \"gemma-scope-9b-pt-res-canonical\", # e.g., \"gpt2-small-res-jb\". See other options in https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n",
    "            sae_id = f\"layer_{layer}/width_131k/canonical\", # e.g., \"blocks.8.hook_resid_pre\". Won't always be a hook point\n",
    "        )\n",
    "    elif sae_type == \"16k\":\n",
    "        sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "            release = \"gemma-scope-9b-pt-res-canonical\", # e.g., \"gpt2-small-res-jb\". See other options in https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n",
    "            sae_id = f\"layer_{layer}/width_16k/canonical\", # e.g., \"blocks.8.hook_resid_pre\". Won't always be a hook point\n",
    "        )\n",
    "    \n",
    "    # for param in sae.parameters():\n",
    "    #     param.data = param.data.to(torch.bfloat16)\n",
    "\n",
    "    sae_dict[layer] = sae.cuda()\n",
    "    for name, param in sae.named_parameters():\n",
    "        param.requires_grad = False\n",
    "print(f\"{torch.cuda.memory_allocated() / 1024 ** 3} GB allocated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_residual_activations(model, target_layers, input_ids, attention_mask=None, unmasked_for_sae=False):\n",
    "  output = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "  hidden_states = output.hidden_states\n",
    "  # hidden state has n_layers + 1 elements, the first one is the embeddings\n",
    "  # so need to add one to the target layer to get the correct layer\n",
    "  if isinstance(target_layers, int):\n",
    "    target_layers = [target_layers]\n",
    "\n",
    "  layer_hidden_states = []\n",
    "  for layer in target_layers:\n",
    "    layer_hidden_states.append(hidden_states[layer+1])\n",
    "  layer_hidden_states = torch.stack(layer_hidden_states)\n",
    "  if unmasked_for_sae:\n",
    "    assert attention_mask is not None, \"Must provide attention mask for flat return\"\n",
    "\n",
    "    # layer hidden states should be shape (n_layers, batch_size, seq_len, hidden_size)\n",
    "    # attention_mask should be shape (batch_size, seq_len)\n",
    "    assert layer_hidden_states.shape[1] == attention_mask.shape[0], f\"Batch size mismatch, {layer_hidden_states.shape} vs {attention_mask.shape}\"\n",
    "    assert layer_hidden_states.shape[2] == attention_mask.shape[1], f\"Sequence length mismatch, {layer_hidden_states.shape} vs {attention_mask.shape}\"\n",
    "    assert len(layer_hidden_states.shape) == 4 and len(attention_mask.shape) == 2, f\"Layer hidden states and attention mask must be 4D and 2D respectively but got {layer_hidden_states.shape} and {attention_mask.shape}\"\n",
    "    batch_indices, seq_indices = torch.where(attention_mask[:, 1:])\n",
    "    layer_hidden_states = layer_hidden_states[:, :, 1:, :]\n",
    "    return layer_hidden_states[:, batch_indices, seq_indices, :]\n",
    "\n",
    "  else:\n",
    "    return layer_hidden_states\n",
    "\n",
    "  # target_act = None\n",
    "  # def gather_target_act_hook(mod, inputs, outputs):\n",
    "  #   nonlocal target_act # make sure we can modify the target_act from the outer scope\n",
    "  #   target_act = outputs[0]\n",
    "  #   return outputs\n",
    "  # handle = model.model.layers[target_layer].register_forward_hook(gather_target_act_hook)\n",
    "  # _ = model.forward(**inputs)\n",
    "  # handle.remove()\n",
    "  # return target_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Update Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'abstract', 'text', 'doi'],\n",
       "    num_rows: 24453\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0f67ce54604da99d42e2ee77200750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c4e8d267d24579ad7f7ba22ce5762e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test dataset available. Using train dataset for testing.\n",
      "{'text': ['It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web. Playing on the web works, but you have to simulate multi-touch for table moving and that can be a bit confusing.\\n\\nThere’s a lot I’d like to talk about. I’ll go through every topic, insted of making the typical what went right/wrong list.\\n\\nConcept\\n\\nWorking over the theme was probably one of the hardest tasks I had to face.\\n\\nOriginally, I had an idea of what kind of game I wanted to develop, gameplay wise – something with lots of enemies/actors, simple graphics, maybe set in space, controlled from a top-down view. I was confident I could fit any theme around it.\\n\\nIn the end, the problem with a theme like “Evolution” in a game is that evolution is unassisted. It happens through several seemingly random mutations over time, with the most apt permutation surviving. This genetic car simulator is, in my opinion, a great example of actual evolution of a species facing a challenge. But is it a game?\\n\\nIn a game, you need to control something to reach an objective. That control goes against what evolution is supposed to be like. If you allow the user to pick how to evolve something, it’s not evolution anymore – it’s the equivalent of intelligent design, the fable invented by creationists to combat the very idea of evolution. Being agnostic and a Pastafarian, that’s not something that rubbed me the right way.\\n\\nHence, my biggest dillema when deciding what to create was not with what I wanted to create, but with what I did not. I didn’t want to create an “intelligent design” simulator and wrongly call it evolution.\\n\\nThis is a problem, of course, every other contestant also had to face. And judging by the entries submitted, not many managed to work around it. I’d say the only real solution was through the use of artificial selection, somehow. So far, I haven’t seen any entry using this at its core gameplay.\\n\\nAlas, this is just a fun competition and after a while I decided not to be as strict with the game idea, and allowed myself to pick whatever I thought would work out.\\n\\nMy initial idea was to create something where humanity tried to evolve to a next level but had some kind of foe trying to stop them from doing so. I kind of had this image of human souls flying in space towards a monolith or a space baby (all based in 2001: A Space Odyssey of course) but I couldn’t think of compelling (read: serious) mechanics for that.\\n\\nBorgs were my next inspiration, as their whole hypothesis fit pretty well into the evolution theme. But how to make it work? Are you the borg, or fighting the Borg?\\n\\nThe third and final idea came to me through my girlfriend, who somehow gave me the idea of making something about the evolution of Pasta. The more I thought about it the more it sounded like it would work, so I decided to go with it.\\n\\nConversations with my inspiring co-worker Roushey (who also created the “Mechanical Underdogs” signature logo for my intros) further matured the concept, as it involved into the idea of having individual pieces of pasta flying around and trying to evolve until they became all-powerful. A secondary idea here was that the game would work to explain how the Flying Spaghetti Monster came to exist – by evolving from a normal dinner table.\\n\\nSo the idea evolved more or less into this: you are sitting a table. You have your own plate, with is your “base”. There are 5 other guests at the table, each with their own plate.\\n\\nYour plate can spawn little pieces of pasta. You do so by “ordering” them through a menu. Some pastas are better than others; some are faster, some are stronger. They have varying costs, which are debited from your credits (you start with a number of credits).\\n\\nOnce spawned, your pastas start flying around. Their instinct is to fly to other plates, in order to conquer them (the objective of the game is having your pasta conquer all the plates on the table). But they are really autonomous, so after being spawned, you have no control over your pasta (think DotA or LoL creeps).\\n\\nYour pasta doesn’t like other people’s pasta, so if they meet, they shoot sauce at each other until one dies. You get credits for other pastas your own pasta kill.\\n\\nOnce a pasta is in the vicinity of a plate, it starts conquering it for its team. It takes around 10 seconds for a plate to be conquered; less if more pasta from the same team are around. If pasta from other team are around, though, they get locked down in their attempt, unable to conquer the plate, until one of them die (think Battlefield’s standard “Conquest” mode).\\n\\nYou get points every second for every plate you own.\\n\\nOver time, the concept also evolved to use an Italian bistro as its main scenario.\\n\\nCarlos, Carlos’ Bistro’s founder and owner\\n\\nSetup\\n\\nNo major changes were made from my work setup. I used FDT and Starling creating an Adobe AIR (ActionScript) project, all tools or frameworks I already had some knowledge with.\\n\\nOne big change for me was that I livestreamed my work through a twitch.tv account. This was a new thing for me. As recommended by Roushey, I used a program called XSplit and I got to say, it is pretty amazing. It made the livestream pretty effortless and the features are awesome, even for the free version. It was great to have some of my friends watch me, and then interact with them and random people through chat. It was also good knowing that I was also recording a local version of the files, so I could make a timelapse video later.\\n\\nKnowing the video was being recorded also made me a lot more self-conscious about my computer use, as if someone was watching over my shoulder. It made me realize that sometimes I spend too much time in seemingly inane tasks (I ended up wasting the longest time just to get some text alignment the way I wanted – it’ll probably drive someone crazy if they watch it) and that I do way too many typos where writing code. I pretty much spend half of the time writing a line and the other half fixing the crazy characters in it.\\n\\nMy own stream was probably boring to watch since I was coding for the most time. But livestreaming is one of the cool things to do as a spectator too. It was great seeing other people working – I had a few tabs opened on my second monitor all the time. It’s actually a bit sad, because if I could, I could have spent the whole weekend just watching other people working! But I had to do my own work, so I’d only do it once in a while, when resting for a bit.\\n\\nDesign\\n\\nAlthough I wanted some simple, low-fi, high-contrast kind of design, I ended up going with somewhat realistic (vector) art. I think it worked very well, fitting the mood of the game, but I also went overboard.\\n\\nFor example: to know the state of a plate (who owns it, who’s conquering it and how much time they have left before conquering it, which pasta units are in the queue, etc), you have to look at the plate’s bill.\\n\\nThe problem I realized when doing some tests is that people never look at the bill! They think it’s some kind of prop, so they never actually read its details.\\n\\nPlus, if you’re zoomed out too much, you can’t actually read it, so it’s hard to know what’s going on with the game until you zoom in to the area of a specific plate.\\n\\nOne other solution that didn’t turn out to be as perfect as I thought was how to indicate who a plate base belongs to. In the game, that’s indicated by the plate’s decoration – its color denotes the team owner. But it’s something that fits so well into the design that people never realized it, until they were told about it.\\n\\nIn the end, the idea of going with a full physical metaphor is one that should be done with care. Things that are very important risk becoming background noise, unless the player knows its importance.\\n\\nOriginally, I wanted to avoid any kind of heads-up display in my game. In the end, I ended up adding it at the bottom to indicate your credits and bases owned, as well as the hideous out-of-place-and-still-not-obvious “Call Waiter” button. But in hindsight, I should have gone with a simple HUD from the start, especially one that indicated each team’s colors and general state of the game without the need for zooming in and out.\\n\\nDevelopment\\n\\nDevelopment went fast. But not fast enough.\\n\\nEven though I worked around 32+ hours for this Ludum Dare, the biggest problem I had to face in the end was overscoping. I had too much planned, and couldn’t get it all done.\\n\\nContent-wise, I had several kinds of pasta planned (Wikipedia is just amazing in that regard), split into several different groups, from small Pastina to huge Pasta al forno. But because of time constraints, I ended up scratching most of them, and ended up with 5 different types of very small pasta – barely something to start when talking about the evolution of Pasta.\\n\\nPastas used in the game. Unfortunately, the macs where never used\\n\\nWhich is one of the saddest things about the project, really. It had the framework and the features to allow an endless number of elements in there, but I just didn’t have time to draw the rest of the assets needed (something I loved to do, by the way).\\n\\nOther non-obvious features had to be dropped, too. For example, when ordering some pasta, you were supposed to select what kind of sauce you’d like with your pasta, each with different attributes. Bolognese, for example, is very strong, but inaccurate; Pesto is very accurate and has great range, but it’s weaker; and my favorite, Vodka, would triggers 10% loss of speed on the pasta hit by it.\\n\\nThe code for that is mostly in there. But in the end, I didn’t have time to implement the sauce selection interface; all pasta ended up using bolognese sauce.\\n\\nTo-do list: lots of things were not done\\n\\nActual programming also took a toll in the development time. Having been programming for a while, I like to believe I got to a point where I know how to make things right, but at the expense of forgetting how to do things wrong in a seemingly good way. What I mean is that I had to take a lot of shortcuts in my code to save time (e.g. a lot of singletons references for cross-communication rather than events or observers, all-encompassing check loops, not fast enough) that left a very sour taste in my mouth. While I know I used to do those a few years ago and survive, I almost cannot accept the state my code is in right now.\\n\\nAt the same time, I do know it was the right thing to do given the timeframe.\\n\\nOne small thing that had some impact was using a somewhat new platform for me. That’s Starling, the accelerated graphics framework I used in Flash. I had tested it before and I knew how to use it well – the API is very similar to Flash itself. However, there were some small details that had some impact during development, making me feel somewhat uneasy the whole time I was writing the game. It was, again, the right thing to do, but I should have used Starling more deeply before (which is the conundrum: I used it for Ludum Dare just so I could learn more about it).\\n\\nArgument and user experience\\n\\nOne final aspect of the game that I learned is that making the game obvious for your players goes a long way into making it fun. If you have to spend the longest time explaining things, your game is doing something wrong.\\n\\nAnd that’s exactly the problem Survival of the Tastiest ultimately faced. It’s very hard for people to understand what’s going on with the game, why, and how. I did have some introductory text at the beginning, but that was a last-minute thing. More importantly, I should have had a better interface or simplified the whole concept so it would be easier for people to understand.\\n\\nThat doesn’t mean the game itself should be simple. It just means that the experience and interface should be approachable and understandable.\\n\\nConclusion\\n\\nI’m extremely happy with what I’ve done and, especially given that this was my first Ludum Dare. However, I feel like I’ve learned a lot of what not to do.\\n\\nThe biggest problem is overscoping. Like Eric Decker said, the biggest lesson we can learn with this is probably with scoping – deciding what to do beforehand in a way you can complete it without having to rush and do something half-assed.\\n\\nI’m sure I will do more Ludum Dares in the future. But if there are any lessons I can take of it, they are to make it simple, to use frameworks and platforms you already have some absolute experience with (otherwise you’ll spend too much time trying to solve easy questions), and to scope for a game that you can complete in one day only (that way, you can actually take two days and make it cool).\\n\\nThis entry was posted\\non Monday, August 27th, 2012 at 10:54 am and is filed under LD #24.\\nYou can follow any responses to this entry through the RSS 2.0 feed.\\nYou can skip to the end and leave a response. Pinging is currently not allowed.\\n\\n3 Responses to ““Survival of the Tastiest” Post-mortem”\\n\\ndarn it , knowing that I missed your livestream makes me a sad panda ;( but more to the point, the game is … well for a startup its original to say the least ;D it has some really neat ideas and more importantly its designed arround touch screens whitch by the looks of the submission is something rare ;o or that could be just me and my short memory -_-! awesum game, love et <3'], 'meta': {'pile_set_name': ['Pile-CC']}}\n"
     ]
    }
   ],
   "source": [
    "# set up dataset\n",
    "batch_size = 1\n",
    "\n",
    "ctx_len = 100\n",
    "bio_ce_task = WMDP_RelearnTask(batch_size=batch_size, tokenizer=tokenizer, corpus=\"bio-forget\", shuffle=True, ctx_length=ctx_len)\n",
    "display(bio_ce_task.dataset)\n",
    "\n",
    "from tasks.general.DatasetTasks import PileTask\n",
    "pile_task = PileTask(batch_size=batch_size, tokenizer=tokenizer, stream_dataset=True, buffer_size=10000, ctx_length=ctx_len)\n",
    "print(pile_task.get_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "\n",
    "class ActivationDataset:\n",
    "    def __init__(self, task_dict, original_model, activation_layers, number_buffer_batches=128, tokenizer=left_tokenizer, text_cols=None, max_length=500, store_in_cpu=True, keep_model_cpu=False, refresh_dict=None):\n",
    "        \"\"\"\n",
    "        activation_layers: 0-indexed list of layers to store activations from\n",
    "\n",
    "        keep_model_cpu: if True, keep the model in CPU memory (so send to cuda on refresh and send back)\n",
    "        refresh_dict: if not None, a dictionary of task name to boolean which is True or False, if True the task's activations will be refreshed\n",
    "        \"\"\"\n",
    "        self.original_model = original_model\n",
    "        self.activation_layers = activation_layers\n",
    "        self.number_buffer_batches = number_buffer_batches\n",
    "        self.buffer = defaultdict(deque)\n",
    "        self.task_dict = task_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        if text_cols is None:\n",
    "            text_cols = \"text\"\n",
    "        if isinstance(text_cols, str):\n",
    "            self.text_cols = {task_name: text_cols for task_name in task_dict.keys()}\n",
    "        else:\n",
    "            self.text_cols = text_cols\n",
    "        self.max_length = max_length\n",
    "        self.store_in_cpu = store_in_cpu\n",
    "        self.keep_model_cpu = keep_model_cpu\n",
    "        self.refresh_dict = refresh_dict\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        # If any buffer is empty, refresh all buffers\n",
    "        if any(len(self.buffer[task_name]) == 0 for task_name in self.task_dict.keys()):\n",
    "            self.refresh()\n",
    "            \n",
    "        # Pop one batch from each task's buffer\n",
    "        try:\n",
    "            batch = {\n",
    "                task_name: self.buffer[task_name].popleft() \n",
    "                for task_name in self.task_dict.keys()\n",
    "            }\n",
    "            return batch\n",
    "        except IndexError:\n",
    "            raise StopIteration(\"Buffer is empty, probably a bug since this should have been refreshed?\")\n",
    "\n",
    "    def refresh(self):\n",
    "        print(\"Refreshing buffer\")\n",
    "        self.original_model.cuda()\n",
    "\n",
    "        # Clear existing buffers\n",
    "        for task_name in self.task_dict.keys():\n",
    "            if len(self.buffer[task_name]) > 0:\n",
    "                print(f\"Clearing nonempty buffer for {task_name}, current len: {len(self.buffer[task_name])}\")\n",
    "            self.buffer[task_name].clear()\n",
    "\n",
    "        # for each task, get number_buffer_batches batches, get input_ids and attention_mask\n",
    "        for task_name, task in self.task_dict.items():\n",
    "            for i in range(self.number_buffer_batches):\n",
    "                # print(f\"{torch.cuda.memory_allocated() / 1024 ** 3} GB allocated\")\n",
    "                batch_text = task.get_batch()[self.text_cols[task_name]]\n",
    "                tokenized = self.tokenizer(batch_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length)\n",
    "                input_ids = tokenized[\"input_ids\"].cuda()\n",
    "                attention_mask = tokenized[\"attention_mask\"].cuda()\n",
    "                if self.refresh_dict is not None and self.refresh_dict[task_name]:\n",
    "                    with torch.no_grad():\n",
    "                        original_output = self.original_model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "                        if self.store_in_cpu:\n",
    "                            original_acts = {layer: original_output.hidden_states[layer+1].cpu() for layer in self.activation_layers}\n",
    "                        else:\n",
    "                            original_acts = {layer: original_output.hidden_states[layer+1] for layer in self.activation_layers}\n",
    "\n",
    "                    if self.store_in_cpu:\n",
    "                        self.buffer[task_name].append({\"input_ids\": input_ids.cpu(), \"attention_mask\": attention_mask.cpu(), \"activations\": original_acts})\n",
    "                    else:\n",
    "                        self.buffer[task_name].append({\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"activations\": original_acts})\n",
    "                else:\n",
    "                    self.buffer[task_name].append({\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n",
    "\n",
    "        # add number_buffer_batches to the buffer\n",
    "        if self.keep_model_cpu:\n",
    "            self.original_model.cpu()\n",
    "        print(\"Buffer refreshed\")\n",
    "\n",
    "\n",
    "activation_layers = [7, 21]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMU/Circuit Breakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c5b91d3e714d6e9252f2b6efece04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning off grads for model.embed_tokens.weight in original model\n",
      "Turning off grads for model.layers.0.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.0.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.0.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.0.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.0.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.0.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.0.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.0.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.0.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.0.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.0.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.1.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.1.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.1.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.1.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.1.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.1.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.1.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.1.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.1.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.1.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.1.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.2.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.2.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.2.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.2.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.2.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.2.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.2.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.2.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.2.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.2.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.2.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.3.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.3.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.3.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.3.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.3.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.3.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.3.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.3.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.3.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.3.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.3.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.4.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.4.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.4.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.4.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.4.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.4.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.4.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.4.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.4.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.4.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.4.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.5.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.5.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.5.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.5.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.5.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.5.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.5.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.5.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.5.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.5.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.5.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.6.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.6.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.6.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.6.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.6.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.6.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.6.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.6.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.6.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.6.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.6.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.7.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.7.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.7.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.7.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.7.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.7.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.7.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.7.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.7.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.7.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.7.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.8.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.8.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.8.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.8.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.8.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.8.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.8.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.8.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.8.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.8.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.8.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.9.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.9.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.9.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.9.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.9.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.9.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.9.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.9.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.9.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.9.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.9.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.10.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.10.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.10.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.10.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.10.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.10.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.10.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.10.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.10.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.10.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.10.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.11.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.11.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.11.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.11.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.11.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.11.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.11.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.11.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.11.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.11.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.11.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.12.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.12.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.12.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.12.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.12.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.12.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.12.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.12.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.12.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.12.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.12.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.13.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.13.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.13.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.13.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.13.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.13.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.13.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.13.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.13.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.13.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.13.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.14.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.14.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.14.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.14.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.14.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.14.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.14.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.14.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.14.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.14.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.14.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.15.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.15.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.15.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.15.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.15.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.15.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.15.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.15.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.15.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.15.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.15.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.16.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.16.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.16.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.16.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.16.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.16.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.16.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.16.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.16.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.16.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.16.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.17.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.17.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.17.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.17.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.17.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.17.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.17.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.17.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.17.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.17.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.17.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.18.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.18.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.18.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.18.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.18.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.18.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.18.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.18.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.18.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.18.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.18.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.19.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.19.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.19.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.19.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.19.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.19.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.19.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.19.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.19.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.19.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.19.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.20.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.20.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.20.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.20.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.20.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.20.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.20.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.20.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.20.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.20.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.20.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.21.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.21.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.21.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.21.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.21.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.21.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.21.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.21.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.21.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.21.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.21.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.22.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.22.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.22.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.22.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.22.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.22.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.22.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.22.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.22.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.22.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.22.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.23.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.23.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.23.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.23.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.23.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.23.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.23.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.23.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.23.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.23.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.23.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.24.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.24.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.24.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.24.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.24.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.24.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.24.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.24.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.24.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.24.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.24.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.25.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.25.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.25.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.25.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.25.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.25.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.25.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.25.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.25.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.25.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.25.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.26.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.26.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.26.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.26.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.26.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.26.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.26.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.26.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.26.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.26.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.26.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.27.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.27.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.27.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.27.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.27.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.27.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.27.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.27.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.27.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.27.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.27.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.28.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.28.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.28.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.28.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.28.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.28.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.28.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.28.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.28.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.28.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.28.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.29.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.29.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.29.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.29.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.29.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.29.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.29.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.29.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.29.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.29.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.29.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.30.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.30.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.30.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.30.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.30.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.30.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.30.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.30.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.30.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.30.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.30.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.31.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.31.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.31.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.31.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.31.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.31.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.31.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.31.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.31.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.31.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.31.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.32.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.32.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.32.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.32.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.32.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.32.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.32.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.32.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.32.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.32.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.32.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.33.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.33.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.33.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.33.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.33.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.33.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.33.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.33.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.33.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.33.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.33.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.34.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.34.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.34.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.34.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.34.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.34.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.34.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.34.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.34.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.34.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.34.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.35.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.35.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.35.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.35.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.35.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.35.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.35.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.35.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.35.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.35.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.35.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.36.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.36.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.36.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.36.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.36.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.36.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.36.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.36.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.36.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.36.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.36.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.37.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.37.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.37.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.37.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.37.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.37.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.37.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.37.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.37.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.37.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.37.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.38.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.38.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.38.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.38.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.38.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.38.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.38.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.38.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.38.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.38.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.38.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.39.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.39.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.39.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.39.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.39.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.39.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.39.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.39.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.39.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.39.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.39.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.40.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.40.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.40.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.40.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.40.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.40.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.40.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.40.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.40.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.40.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.40.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.41.self_attn.q_proj.weight in original model\n",
      "Turning off grads for model.layers.41.self_attn.k_proj.weight in original model\n",
      "Turning off grads for model.layers.41.self_attn.v_proj.weight in original model\n",
      "Turning off grads for model.layers.41.self_attn.o_proj.weight in original model\n",
      "Turning off grads for model.layers.41.mlp.gate_proj.weight in original model\n",
      "Turning off grads for model.layers.41.mlp.up_proj.weight in original model\n",
      "Turning off grads for model.layers.41.mlp.down_proj.weight in original model\n",
      "Turning off grads for model.layers.41.input_layernorm.weight in original model\n",
      "Turning off grads for model.layers.41.post_attention_layernorm.weight in original model\n",
      "Turning off grads for model.layers.41.pre_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.layers.41.post_feedforward_layernorm.weight in original model\n",
      "Turning off grads for model.norm.weight in original model\n"
     ]
    }
   ],
   "source": [
    "lora = False\n",
    "\n",
    "if pretrained_path is not None:\n",
    "    train_model = AutoModelForCausalLM.from_pretrained(pretrained_path, torch_dtype=dtype)\n",
    "else:\n",
    "    train_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype)\n",
    "train_model.cuda()\n",
    "model.eval()\n",
    "model.cpu()\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Turning off grads for {name} in original model\")\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.0.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.0.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.0.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.1.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.1.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.1.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.2.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.2.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.2.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.3.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.3.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.3.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.4.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.4.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.4.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.5.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.5.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.5.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.6.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.6.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.6.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.7.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.7.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.7.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.8.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.8.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.8.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.9.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.9.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.9.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.10.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.10.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.10.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.11.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.11.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.11.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.12.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.12.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.12.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.13.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.13.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.13.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.14.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.14.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.14.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.15.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.15.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.15.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.16.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.16.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.16.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.17.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.17.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.17.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.18.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.18.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.18.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.19.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.19.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.19.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.20.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.20.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.20.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.21.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.21.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.21.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.22.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.22.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.22.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.23.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.23.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.23.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.24.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.24.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.24.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.25.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.25.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.25.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.26.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.26.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.26.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 3584])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([2048, 3584])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([3584, 4096])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([14336, 3584])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([3584, 14336])\n",
      "model.layers.27.input_layernorm.weight torch.Size([3584])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([3584])\n",
      "model.layers.27.pre_feedforward_layernorm.weight torch.Size([3584])\n",
      "model.layers.27.post_feedforward_layernorm.weight torch.Size([3584])\n",
      "Number of trainable parameters: 5.5494656 billion\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "# layers_to_transform = list(range(n_layers))\n",
    "layers_to_transform = list(range(28))\n",
    "if lora:\n",
    "    config = LoraConfig(\n",
    "        r=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "        layers_to_transform=layers_to_transform,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05\n",
    "    )\n",
    "    train_model = get_peft_model(train_model, config)\n",
    "\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "else:\n",
    "    for name, param in train_model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    for layer in range(n_layers):\n",
    "        if layer in layers_to_transform:\n",
    "            for param in train_model.model.layers[layer].parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in train_model.model.layers[layer].parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    learning_rate = 2e-5\n",
    "\n",
    "# how many trainable parameters?\n",
    "trainable_params = 0\n",
    "for name, param in train_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)\n",
    "        trainable_params += param.numel()\n",
    "print(f\"Number of trainable parameters: {trainable_params / 1e9} billion\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restrict Gradient Updates using SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sae_acts(layer_acts, sae_layers, sae_dict, feature_dict, apply_nonlinearity=True):\n",
    "\n",
    "    all_feature_acts = {}\n",
    "    for layer in sae_layers:\n",
    "        features = feature_dict[layer]\n",
    "        sae = sae_dict[layer]\n",
    "        feature_w = sae.W_enc[:, features] # [hidden, n_features]\n",
    "        feature_b = sae.b_enc[features] # [n_features]\n",
    "        threshold = sae.threshold[features] # [n_features]\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=dtype):\n",
    "            feature_acts = einops.einsum(layer_acts[layer], feature_w, \"... hidden, hidden n_features -> ... n_features\") + feature_b\n",
    "            if apply_nonlinearity:\n",
    "                feature_acts = torch.where(feature_acts > threshold, feature_acts, 0)\n",
    "            all_feature_acts[layer] = feature_acts\n",
    "    return all_feature_acts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricting Gradients on Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf3c94f0d7b48ef89afbe2576eb9698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5a75547e0a4d2685fddbf1bfac95a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<bound method WMDP_UnlearnMCTask.format_row of <tasks.wmdp.WMDP_UnlearnTask.WMDP_UnlearnMCTask object at 0x7f73d4372f90>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fa5d844bc94cb78d7f0026723fb6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d2e6280346494fa97509c5bf63e3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecf499842e745dd98113020c1b98996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2d2beb242e4fda819298122abf49dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8a07f12e5e46fe9953797334fe7710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60e5ee6afe74ec292c741fdc35e86c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57109c2e1e204bbca3ad1e94361792f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025bbe4a523643688e456e180b5d2ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687dc8280ab640749193a7063f83a4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1819e741443b4821be4932e709b68bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb6d4172041461d8e816e2c2ba06078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0635fa441722464aa992e55cf001a5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test dataset available. Using train dataset for testing.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d0446439fd41c7be20cbfea0f72c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c53d3b16e54d48b88fe281d22a47a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcf5197556049c9ab72de8f0d1e73fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237ca7f40cfe4dc2a7a8050973180fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359371f5ab4542ad852a94bc22cc0028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d3e672259e4f37967346ba2dad6261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04af3b0ccd949dbbd4eefbc7b14ad44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f36e3af4c54f3baa7c86d75596a1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evaluation iterations for MC: 16\n"
     ]
    }
   ],
   "source": [
    "# log 1 minus p on forget corpus\n",
    "batch_size = 1\n",
    "ctx_len = 100\n",
    "from tasks.wmdp.WMDP_MCTask import WMDP_DedupedTask, WMDP_MCTask\n",
    "from tasks.wmdp.WMDP_UnlearnTask import WMDP_UnlearnTask, WMDP_UnlearnMCTask\n",
    "\n",
    "activation_layers = list(range(22))\n",
    "# activation_layers = [7, 21]\n",
    "\n",
    "# k = 2\n",
    "# sae_layers = [7, 21]\n",
    "n_features = 10\n",
    "pile_clamp_thresholds = {7: 0.05, 14: .05, 21: 0.03, 35: 0.01}\n",
    "num_batches = 200\n",
    "mask_top_quantile = .99\n",
    "sample_frac = 0.05\n",
    "\n",
    "# mask_task = WMDP_RelearnTask(batch_size=batch_size, tokenizer=tokenizer, corpus=\"bio-forget\", shuffle=True)\n",
    "mask_task = WMDP_UnlearnMCTask(batch_size=batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True, split=\"first_two\", train_test_split=False, injection_task=False)\n",
    "task_type = \"fact_unlearn\"\n",
    "model.cpu()\n",
    "# v_projections = get_v_projections(model, task, activation_layers, sae_layers, n_features, pile_clamp_thresholds, num_batches, k)\n",
    "# v_projections['model.layers.7.self_attn.q_proj.weight'].shape\n",
    "\n",
    "tasks = {\"bio\": WMDP_UnlearnMCTask(batch_size=batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=False, split=\"first_two\", train_test_split=False, criterion=\"log_1_minus_p\", injection_task=False, model_type=model_type, filter_correct_prob_threshold=0.5),\n",
    "         \"bio_injection\": WMDP_UnlearnMCTask(batch_size=batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=False, split=\"first_two\", train_test_split=False, criterion=\"cross_entropy\", injection_task=True, model_type=model_type, filter_correct_prob_threshold=0.5),\n",
    "         \"pile\": PileTask(batch_size=batch_size, tokenizer=tokenizer, stream_dataset=True, buffer_size=10000, ctx_length=ctx_len)\n",
    "         }\n",
    "eval_batch_size = 8\n",
    "bio_mc_task = WMDP_UnlearnMCTask(batch_size=eval_batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True, split=\"first_two\", train_test_split=False, injection_task=False, model_type=model_type, filter_correct_prob_threshold=0.5)\n",
    "bio_injection_mc_task = WMDP_UnlearnMCTask(batch_size=eval_batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True, split=\"first_two\", train_test_split=False, injection_task=True, model_type=model_type, filter_correct_prob_threshold=0.5)\n",
    "num_eval_iters = len(bio_mc_task.test_dataset) // eval_batch_size * 2\n",
    "print(f\"Number of evaluation iterations for MC: {num_eval_iters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model, refreshing masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(model, bio_mc_batch, mask_top_quantile=.999, sample_frac=1):\n",
    "    # batch should probably be size 1\n",
    "    # assumes 0 grad beforehand\n",
    "    original_grads = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        # assert param.grad is None, f\"For now get_masks is only supported when gradients start as 0, but got {name} with grad {param.grad}\"\n",
    "        if param.grad is not None:\n",
    "            original_grads[name] = param.grad\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    assert len(bio_mc_batch[\"question\"]) == 1\n",
    "    feature_dict = json.loads(bio_mc_batch[\"per_prompt_related_features\"][0])\n",
    "    feature_dict = {int(k): v for k, v in feature_dict.items()}\n",
    "\n",
    "    tokenized = tokenizer(bio_mc_batch[\"prompt\"], return_tensors=\"pt\")\n",
    "    output = model(tokenized[\"input_ids\"].cuda(), attention_mask=tokenized[\"attention_mask\"].cuda(), output_hidden_states=True)\n",
    "    acts = {layer: output.hidden_states[layer+1][0, -1] for layer in sae_layers}\n",
    "    sae_acts = get_sae_acts(acts, sae_layers, sae_dict, feature_dict)\n",
    "    # print(f\"{sae_acts=}\")\n",
    "    total_loss = 0\n",
    "    for sae_layer in sae_layers:\n",
    "        total_loss += sae_acts[sae_layer].sum()\n",
    "    total_loss.backward()\n",
    "\n",
    "\n",
    "    all_grads = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            grad = param.grad.abs().flatten()\n",
    "            if sample_frac < 1:\n",
    "                # grad = grad[torch.randperm(len(grad))[:int(len(grad) * sample_frac)]]\n",
    "                grad = grad[:int(len(grad) * sample_frac)]\n",
    "            all_grads.append(grad.float())\n",
    "    all_grads = torch.cat(all_grads)\n",
    "\n",
    "    # Calculate single threshold across all gradients\n",
    "    n = all_grads.numel()\n",
    "    k = int(n * mask_top_quantile)\n",
    "    \n",
    "    threshold = torch.kthvalue(all_grads, k).values.item()\n",
    "\n",
    "    # all_grads.cpu()\n",
    "    # print(f\"{all_grads.shape=}, n: {n}, k: {k}, threshold: {threshold}\")\n",
    "    # del all_grads\n",
    "\n",
    "    # Create masks using the single threshold\n",
    "    masks = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            masks[name] = (param.grad.abs() > threshold)\n",
    "\n",
    "    model.zero_grad()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in original_grads:\n",
    "            param.grad = original_grads[name]\n",
    "    return masks\n",
    "\n",
    "def apply_gradient_masks(model, masks):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            if name in masks:\n",
    "                # print(f\"Applying mask to gradient {name}\")\n",
    "                param.grad = param.grad * masks[name]\n",
    "            else:\n",
    "                print(f\"Gradient mask for {name} not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 35.59472894668579 GB allocated\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before training: {torch.cuda.memory_allocated() / 1024 ** 3} GB allocated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilliphguo\u001b[0m (\u001b[33mquirky_lats_at_mats\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/sae-editing/wandb/run-20250107_190322-ow5n6323</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/quirky_lats_at_mats/sae-unlearning/runs/ow5n6323' target=\"_blank\">peachy-snowball-102</a></strong> to <a href='https://wandb.ai/quirky_lats_at_mats/sae-unlearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/quirky_lats_at_mats/sae-unlearning' target=\"_blank\">https://wandb.ai/quirky_lats_at_mats/sae-unlearning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/quirky_lats_at_mats/sae-unlearning/runs/ow5n6323' target=\"_blank\">https://wandb.ai/quirky_lats_at_mats/sae-unlearning/runs/ow5n6323</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5506f5970c1484db20cfb5be0f405d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n",
      "Applying gradient masks\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.21 GiB of which 38.75 MiB is free. Process 2947643 has 79.16 GiB memory in use. Of the allocated memory 76.76 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m bio_injection_batch \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbio_injection\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget_batch()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m bio_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m bio_injection_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBio batch question \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbio_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match injection batch question \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbio_injection_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 71\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[43mget_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbio_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_top_quantile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_top_quantile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_frac\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m current_rerouting_loss \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbio\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcalculate_loss(train_model, bio_batch)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Scale loss and backward\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 51\u001b[0m, in \u001b[0;36mget_masks\u001b[0;34m(model, bio_mc_batch, mask_top_quantile, sample_frac)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m---> 51\u001b[0m         masks[name] \u001b[38;5;241m=\u001b[39m (\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m)\n\u001b[1;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.21 GiB of which 38.75 MiB is free. Process 2947643 has 79.16 GiB memory in use. Of the allocated memory 76.76 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "n_steps = 500\n",
    "\n",
    "\n",
    "mask_top_quantile = .99\n",
    "sample_frac = 0.01\n",
    "\n",
    "# grad_accum_steps = 4\n",
    "evaluate_every = 10\n",
    "# refresh_masks_every = 100\n",
    "\n",
    "grad_accum_steps = 16 // batch_size\n",
    "learning_rate = 2e-5\n",
    "\n",
    "if lora:\n",
    "    optimizer = torch.optim.AdamW(train_model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "else:\n",
    "    import bitsandbytes as bnb\n",
    "    optimizer = bnb.optim.AdamW8bit(train_model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=n_steps)\n",
    "\n",
    "circuit_breaker_coef = 1\n",
    "\n",
    "do_gradient_masking = True\n",
    "# apply_mask_to_retain = True\n",
    "\n",
    "# Initialize wandb run before training\n",
    "wandb.init(\n",
    "    project=\"sae-unlearning\",\n",
    "    config={\n",
    "        \"unlearning_method\": \"masked_gradient_descent_mc_data\" if do_gradient_masking else \"gradient_descent_mc_data\",\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"n_steps\": n_steps,\n",
    "        \"lora\": lora,\n",
    "        \"activation_layers\": activation_layers,\n",
    "        \"model\": model_name_or_path,\n",
    "        \"circuit_breaker_coef\": circuit_breaker_coef,\n",
    "        # \"refresh_masks_every\": refresh_masks_every,\n",
    "        # \"apply_mask_to_retain\": apply_mask_to_retain,\n",
    "        \"sae_layers\": sae_layers,\n",
    "        \"n_sae_features\": n_features,\n",
    "        \"mask_top_quantile\": mask_top_quantile,\n",
    "        \"sample_frac\": sample_frac,\n",
    "        # \"task_type\": task_type\n",
    "    }\n",
    ")\n",
    "\n",
    "for step in tqdm(range(n_steps)):\n",
    "    # if step % refresh_masks_every == 0 and do_gradient_masking:\n",
    "    #     print(\"Refreshing masks\")\n",
    "    #     masks = get_masks(train_model, mask_task, activation_layers, sae_layers, n_features, pile_clamp_thresholds, num_batches, mask_top_quantile=mask_top_quantile, sample_frac=sample_frac, task_type=task_type)\n",
    "    #     print(\"Done refreshing masks\")\n",
    "    # Track per-layer losses for logging\n",
    "    total_rerouting_loss = 0\n",
    "    total_injection_rerouting_loss = 0\n",
    "    # total_cyber_retain_loss = 0\n",
    "    total_retain_loss = 0\n",
    "    \n",
    "    # Calculate coefficients\n",
    "    progress = step / n_steps\n",
    "    progress = progress ** 2\n",
    "    retain_coef = 1\n",
    "\n",
    "\n",
    "    # first do rerouting, mask\n",
    "    for accum_step in range(grad_accum_steps):\n",
    "        \n",
    "        bio_batch = tasks[\"bio\"].get_batch()\n",
    "        bio_injection_batch = tasks[\"bio_injection\"].get_batch()\n",
    "        assert bio_batch[\"question\"] == bio_injection_batch[\"question\"], f\"Bio batch question {bio_batch['question']} does not match injection batch question {bio_injection_batch['question']}\"\n",
    "\n",
    "        masks = get_masks(train_model, bio_batch, mask_top_quantile=mask_top_quantile, sample_frac=sample_frac)\n",
    "\n",
    "        current_rerouting_loss = tasks[\"bio\"].calculate_loss(train_model, bio_batch)\n",
    "        \n",
    "        # Scale loss and backward\n",
    "        (circuit_breaker_coef * current_rerouting_loss / grad_accum_steps).backward()\n",
    "        total_rerouting_loss += current_rerouting_loss.item() / grad_accum_steps\n",
    "\n",
    "        current_injection_rerouting_loss = tasks[\"bio_injection\"].calculate_loss(train_model, bio_injection_batch)\n",
    "        (circuit_breaker_coef * current_injection_rerouting_loss / grad_accum_steps).backward()\n",
    "        total_injection_rerouting_loss += current_injection_rerouting_loss.item() / grad_accum_steps\n",
    "        if do_gradient_masking:\n",
    "            print(f\"Applying gradient masks\")\n",
    "            apply_gradient_masks(train_model, masks)\n",
    "        \n",
    "        # print(f\"{torch.cuda.memory_allocated() / 1024 ** 3} GB allocated\")\n",
    "\n",
    "    # then do unmasked retain\n",
    "    # Gradient accumulation loop\n",
    "    for accum_step in range(grad_accum_steps):\n",
    "\n",
    "        # Retain (cyber) forward pass\n",
    "        # current_cyber_retain_loss = tasks[\"cyber\"].get_train_loss(train_model)\n",
    "        \n",
    "        # # Scale loss and backward\n",
    "        # (retain_coef * current_cyber_retain_loss / grad_accum_steps).backward()\n",
    "        # total_cyber_retain_loss += current_cyber_retain_loss.item() / grad_accum_steps\n",
    "\n",
    "        # Retain (pile) forward pass\n",
    "        current_retain_loss = tasks[\"pile\"].get_train_loss(train_model)\n",
    "        \n",
    "        # Scale loss and backward\n",
    "        (retain_coef * current_retain_loss / grad_accum_steps).backward()\n",
    "        total_retain_loss += current_retain_loss.item() / grad_accum_steps\n",
    "\n",
    "    # if apply_mask_to_retain and do_gradient_masking:\n",
    "    #     apply_gradient_masks(train_model, masks)\n",
    "    # Update weights after accumulation\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "\n",
    "    eval_metrics = {}\n",
    "    if (step+1) % evaluate_every == 0:\n",
    "        print(\"Evaluating\")\n",
    "        # bio_loss = tasks[\"bio\"].get_test_loss(train_model, n_iters=grad_accum_steps)\n",
    "        # print(f\"Bio loss: {bio_loss:.3f}\")\n",
    "        bio_mc_acc = bio_mc_task.get_test_accuracy(train_model, n_iters=num_eval_iters)\n",
    "        print(f\"Bio MC accuracy: {bio_mc_acc:.3f}\")\n",
    "        injection_bio_mc_acc = bio_injection_mc_task.get_test_accuracy(train_model, n_iters=num_eval_iters)\n",
    "        print(f\"Injection Bio MC accuracy: {injection_bio_mc_acc:.3f}\")\n",
    "        mmlu = run_general_evals(train_model, model_type=model_type, evals_to_include=[\"MMLU\"], verbose=False, batch_size=2, device=\"cuda\")[\"MMLU\"]\n",
    "        print(f\"MMLU: {mmlu:.3f}\")\n",
    "        # eval_metrics[\"bio_loss\"] = bio_loss\n",
    "        eval_metrics[\"bio_mc_acc\"] = bio_mc_acc\n",
    "        eval_metrics[\"injection_bio_mc_acc\"] = injection_bio_mc_acc\n",
    "        eval_metrics[\"mmlu\"] = mmlu\n",
    "\n",
    "    # total_loss = circuit_breaker_coef * total_rerouting_loss + retain_coef * (total_cyber_retain_loss + total_retain_loss)\n",
    "    total_loss = circuit_breaker_coef * total_rerouting_loss + circuit_breaker_coef * total_injection_rerouting_loss + retain_coef * total_retain_loss\n",
    "\n",
    "    # Evaluation and logging code...\n",
    "    wandb.log({\n",
    "        \"loss/total\": total_loss,\n",
    "        \"loss/rerouting\": total_rerouting_loss,\n",
    "        \"loss/injection_rerouting\": total_injection_rerouting_loss,\n",
    "        \"loss/retain\": total_retain_loss,\n",
    "        # \"loss/cyber_retain\": total_cyber_retain_loss,\n",
    "        \"progress\": progress,\n",
    "        \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "        **eval_metrics\n",
    "    }, step=step)\n",
    "\n",
    "# Close wandb run when done\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.0.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.0.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.0.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.1.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.1.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.1.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.2.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.2.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.2.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.3.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.3.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.3.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.4.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.4.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.4.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.5.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.5.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.5.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.6.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.6.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.6.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.7.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.7.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.7.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.8.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.8.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.8.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.9.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.9.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.9.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.10.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.10.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.10.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.11.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.11.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.11.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.12.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.12.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.12.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.13.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.13.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.13.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.14.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.14.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.14.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.15.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.15.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.15.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.16.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.16.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.16.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.17.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.17.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.17.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.18.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.18.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.18.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.19.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.19.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.19.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.20.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.20.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.20.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.21.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.21.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.21.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.22.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.22.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.22.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.23.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.23.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.23.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.24.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.24.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.24.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.25.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.25.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.25.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.26.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.26.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.26.post_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 3584]) True\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([2048, 3584]) True\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([3584, 4096]) True\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([14336, 3584]) True\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([3584, 14336]) True\n",
      "model.layers.27.input_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.27.pre_feedforward_layernorm.weight torch.Size([3584]) True\n",
      "model.layers.27.post_feedforward_layernorm.weight torch.Size([3584]) True\n"
     ]
    }
   ],
   "source": [
    "for name, param in train_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape, param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to gemma-2-sae-masked-gd-mc-6-fullrank\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f77e2af402244248b95153ada259de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7838a40a406b43af939b1cda2e9a8dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c956a87c4ea140ef9cc2efa88f644173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469b98700ed247909484b08a3e5e22b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3cd7be71c64c659cca55c1bb1d0e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/PhillipGuo/gemma-2-sae-masked-gd-mc-6-fullrank/commit/53e68a25535aa75b35ddb0bd91b95d8d27eb3c23', commit_message='500 steps, wandb_run = https://wandb.ai/quirky_lats_at_mats/sae-unlearning/runs/iqxggr58/workspace?nw=nwuserphilliphguo', commit_description='', oid='53e68a25535aa75b35ddb0bd91b95d8d27eb3c23', pr_url=None, repo_url=RepoUrl('https://huggingface.co/PhillipGuo/gemma-2-sae-masked-gd-mc-6-fullrank', endpoint='https://huggingface.co', repo_type='model', repo_id='PhillipGuo/gemma-2-sae-masked-gd-mc-6-fullrank'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save peft on hf\n",
    "run_id = 7\n",
    "name = model_type + (\"-sae-masked-gd-mc\" if do_gradient_masking else \"-gd-mc\") + f\"-{run_id}\" + (\"-lora\" if lora else \"-fullrank\")\n",
    "print(f\"Saving to {name}\")\n",
    "train_model.save_pretrained(name)\n",
    "train_model.push_to_hub(\n",
    "    name, \n",
    "    # use_auth_token=True,\n",
    "    commit_message=\"500 steps, wandb_run = https://wandb.ai/quirky_lats_at_mats/sae-unlearning/runs/iqxggr58/workspace?nw=nwuserphilliphguo\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
