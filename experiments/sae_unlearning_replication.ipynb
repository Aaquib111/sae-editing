{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Replicate the \"Applying Sparse Autoencoders to Unlearn Knowledge in Language Models\" paper\n",
    "\n",
    "Link: https://arxiv.org/pdf/2410.19278\n",
    "\n",
    "Figures of interest:\n",
    "- Figure 2: Max activating examples for feature #9163 on OpenWebText and WMDP-bio\n",
    "\n",
    "- Figure 4: Probabilities of answering A, B, C or D for question #841 as a fuction of clamping #9163\n",
    "\n",
    "- Figure 5: MMLU vs MWDP-Bio Acc across different number of intervened features and different clamping multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"cais/wmdp\", \"wmdp-bio\")\n",
    "owt = load_dataset(\"Skylion007/openwebtext\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-p4-mlp\", # e.g., \"gpt2-small-res-jb\". See other options in https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n",
    "    sae_id = \"blocks.9.hook_mlp_out\", # e.g., \"blocks.8.hook_resid_pre\". Won't always be a hook point\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
