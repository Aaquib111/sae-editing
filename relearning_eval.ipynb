{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/sae-editing\n",
      "Successfully authenticated with Hugging Face.\n",
      "Successfully authenticated with Weights & Biases.\n"
     ]
    }
   ],
   "source": [
    "# from circuit_breaking.src import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import contextlib\n",
    "import einops\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import bitsandbytes as bnb\n",
    "from collections import defaultdict\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "HF_ACCESS_TOKEN = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "if HF_ACCESS_TOKEN:\n",
    "    login(token=HF_ACCESS_TOKEN)\n",
    "    print(\"Successfully authenticated with Hugging Face.\")\n",
    "else:\n",
    "    print(\"Hugging Face access token not found in environment variables.\")\n",
    "\n",
    "if WANDB_API_KEY:\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "    print(\"Successfully authenticated with Weights & Biases.\")\n",
    "else:\n",
    "    print(\"Weights & Biases API key not found in environment variables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954b8a80b24f42d18b47f8d5719e3c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = \"google/gemma-2-9b\"\n",
    "model_type = \"gemma-2\"\n",
    "other_model_type = \"gemma2_9b\"\n",
    "# pretrained_path = \"/data/huggingface/models--google--gemma-2-9b/snapshots/33c193028431c2fde6c6e51f29e6f17b60cbfac6/\"\n",
    "# pretrained_path = \"/data/huggingface/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/\"\n",
    "# pretrained_path = \"gemma-2-rmu-fullrank\"\n",
    "# pretrained_path = \"PhillipGuo/gemma-2-rmu-fullrank\"\n",
    "# pretrained_path = \"PhillipGuo/gemma-2-sae-cb-fullrank\"\n",
    "pretrained_path = \"gemma-2-gd-mc-fullrank\"\n",
    "# pretrained_path = \"PhillipGuo/gemma-2-9b-rmu-lora-2\"\n",
    "# pretrained_path = \"PhillipGuo/gemma-2-9b-sae-cb-lora\"\n",
    "is_lora = \"lora\" in pretrained_path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "left_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "left_tokenizer.pad_token_id = left_tokenizer.eos_token_id\n",
    "left_tokenizer.padding_side = \"left\"\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "if is_lora:\n",
    "    from peft import AutoPeftModel\n",
    "    if pretrained_path is not None:\n",
    "        model = AutoPeftModel.from_pretrained(pretrained_path, torch_dtype=dtype)\n",
    "    else:\n",
    "        model = AutoPeftModel.from_pretrained(model_name_or_path, torch_dtype=dtype)\n",
    "    model = model.merge_and_unload()\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "else:\n",
    "    if pretrained_path is not None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(pretrained_path, torch_dtype=dtype)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype)\n",
    "model.cuda()\n",
    "n_layers = 42\n",
    "n_heads = 16\n",
    "n_kv_heads = 8\n",
    "\n",
    "param_count_dict = {\"attn.hook_q\": 3584*4096, \"attn.hook_k\": 3584*2048, \"attn.hook_v\": 3584*2048, \"attn.hook_result\": 4096*3584, \"mlp.hook_pre\": 3584 * 14336, \"mlp.hook_post\": 14336 * 3584, \"mlp.hook_gate\": 3584 * 14336}\n",
    "mmlu_batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.wmdp.WMDP_MCTask import WMDP_MCTask, WMDP_DedupedTask\n",
    "from tasks.wmdp.WMDP_RelearnTask import WMDP_RelearnTask\n",
    "from tasks.general_capabilities.MCTask_redo import run_general_evals\n",
    "batch_size = 8\n",
    "bio_mc_task = WMDP_MCTask(batch_size=batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True)\n",
    "num_iters = len(bio_mc_task.dataset) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f67eb4598c4682a937ce7792f892b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/127k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd581d4df0ab4c88b22a5887e09e0c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/38.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ef5c707c0f4644b8cad7b8ed152881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df50a42e28c4eaeb07b4619b5de923e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/71 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b0ddaa3822483d89f5db5fcd491c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52c1b3aab1443a8b8c94f9799219335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/71 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.235791015625\n",
      "tensor(7.8125, device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "bio_mc_deduped_task = WMDP_DedupedTask(batch_size=batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True)\n",
    "num_train_iters = len(bio_mc_deduped_task.train_dataset) // batch_size\n",
    "num_test_iters = (len(bio_mc_deduped_task.test_dataset) * 4) // batch_size\n",
    "\n",
    "print(bio_mc_deduped_task.get_test_accuracy(model, use_test_data=True, num_iters=num_test_iters, continuous=True))\n",
    "print(bio_mc_deduped_task.get_test_loss(model, n_iters=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23571428741727557\n"
     ]
    }
   ],
   "source": [
    "print(bio_mc_deduped_task.get_test_accuracy(model, use_test_data=True, num_iters=num_test_iters, continuous=False))\n",
    "# print(bio_mc_deduped_task.get_test_accuracy(model, use_test_data=True, num_iters=num_test_iters, continuous=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# original_bio_df = load_dataset(\"cais/wmdp\", \"wmdp-bio\", split=\"test\").to_pandas()\n",
    "# original_cyber_df = load_dataset(\"cais/wmdp\", \"wmdp-cyber\", split=\"test\").to_pandas()\n",
    "\n",
    "# # load in deduped data, split into train and test\n",
    "# bio_train_dfs = []\n",
    "# cyber_train_dfs = []\n",
    "# for train_split_idx in [0, 1, 2, 3]:\n",
    "#     # tasks/wmdp/data/mcq_split_0.jsonl\n",
    "#     mcq_formatted_data = pd.read_json(f\"tasks/wmdp/data/mcq_split_{train_split_idx}.jsonl\", lines=True)\n",
    "#     bio_indices = mcq_formatted_data[\"question\"].isin(original_bio_df[\"question\"])\n",
    "#     cyber_indices = mcq_formatted_data[\"question\"].isin(original_cyber_df[\"question\"])\n",
    "#     # display(bio_indices + cyber_indices)\n",
    "#     # display(mcq_formatted_data[~(bio_indices + cyber_indices)])\n",
    "#     print(\"Bio dataset is size \", len(mcq_formatted_data[bio_indices]), \"and cyber dataset is size \", len(mcq_formatted_data[cyber_indices]), \"Missed \", len(mcq_formatted_data[~(bio_indices + cyber_indices)]), \"data points in train split \", train_split_idx)\n",
    "#     bio_train_dfs.append(mcq_formatted_data[bio_indices])\n",
    "#     cyber_train_dfs.append(mcq_formatted_data[cyber_indices])\n",
    "\n",
    "# bio_test_dfs = []\n",
    "# cyber_test_dfs = []\n",
    "# for test_split_idx in [4]:\n",
    "#     mcq_formatted_data = pd.read_json(f\"tasks/wmdp/data/mcq_split_{test_split_idx}.jsonl\", lines=True)\n",
    "#     bio_indices = mcq_formatted_data[\"question\"].isin(original_bio_df[\"question\"])\n",
    "#     cyber_indices = mcq_formatted_data[\"question\"].isin(original_cyber_df[\"question\"])\n",
    "#     bio_test_dfs.append(mcq_formatted_data[bio_indices])\n",
    "#     cyber_test_dfs.append(mcq_formatted_data[cyber_indices])\n",
    "#     print(\"Bio dataset is size \", len(mcq_formatted_data[bio_indices]), \"and cyber dataset is size \", len(mcq_formatted_data[cyber_indices]), \"Missed \", len(mcq_formatted_data[~(bio_indices + cyber_indices)]), \"data points in test split \", test_split_idx)\n",
    "\n",
    "# bio_train_df = pd.concat(bio_train_dfs, ignore_index=True)\n",
    "# cyber_train_df = pd.concat(cyber_train_dfs, ignore_index=True)\n",
    "# bio_test_df = pd.concat(bio_test_dfs, ignore_index=True)\n",
    "# cyber_test_df = pd.concat(cyber_test_dfs, ignore_index=True)\n",
    "# from datasets import Dataset, DatasetDict\n",
    "# # convert to huggingface dataset\n",
    "# bio_train_dataset = Dataset.from_pandas(bio_train_df)\n",
    "# cyber_train_dataset = Dataset.from_pandas(cyber_train_df)\n",
    "# bio_test_dataset = Dataset.from_pandas(bio_test_df)\n",
    "# cyber_test_dataset = Dataset.from_pandas(cyber_test_df)\n",
    "\n",
    "# # Create separate DatasetDicts for bio and cyber\n",
    "# bio_dataset = DatasetDict({\n",
    "#     \"train\": bio_train_dataset,\n",
    "#     \"test\": bio_test_dataset\n",
    "# })\n",
    "\n",
    "# cyber_dataset = DatasetDict({\n",
    "#     \"train\": cyber_train_dataset,\n",
    "#     \"test\": cyber_test_dataset\n",
    "# })\n",
    "# bio_dataset.push_to_hub(\"PhillipGuo/wmdp-deduped\", \"wmdp-bio\")\n",
    "# cyber_dataset.push_to_hub(\"PhillipGuo/wmdp-deduped\", \"wmdp-cyber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "def do_relearning(model, train_tasks, n_iters, grad_accum_steps=8, finetune_lora=False, lora_kwargs={'rank': 256, 'alpha': 32, 'dropout': 0.05, 'target_modules': 'all-linear'}, learning_kwargs={'lr': 1e-5, 'weight_decay': 0, 'use_cosine': False}, eval_callback_fn=None):\n",
    "    # can either finetune full or lora\n",
    "\n",
    "    if not finetune_lora:\n",
    "        optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=learning_kwargs['lr'], weight_decay=learning_kwargs['weight_decay'])\n",
    "\n",
    "    elif finetune_lora:\n",
    "        peft_config = LoraConfig(\n",
    "            inference_mode=False,\n",
    "            r=lora_kwargs['rank'],\n",
    "            lora_alpha=lora_kwargs['alpha'],\n",
    "            lora_dropout=lora_kwargs['dropout'],\n",
    "            target_modules = lora_kwargs['target_modules'], #[\"q_proj\", \"v_proj\", \n",
    "        )\n",
    "\n",
    "        model = get_peft_model(model, peft_config).cuda()\n",
    "        # model.print_trainable_parameters()\n",
    "        print(f\"Parameters in peft: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_kwargs['lr'], weight_decay=learning_kwargs['weight_decay'])\n",
    "    \n",
    "    if learning_kwargs['use_cosine']:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=n_iters)\n",
    "\n",
    "    train_losses = defaultdict(list)\n",
    "    test_losses = []\n",
    "\n",
    "    for iter_idx in tqdm(range(n_iters)):\n",
    "        log_dict = {}\n",
    "        optimizer.zero_grad()\n",
    "        for task_name, (task, task_weight) in train_tasks.items():\n",
    "            task_loss = 0\n",
    "            for i in range(grad_accum_steps):\n",
    "                loss = task.get_train_loss(model) / grad_accum_steps\n",
    "                task_loss += loss.item()\n",
    "                (loss * task_weight).backward()\n",
    "            train_losses[task_name].append(task_loss)\n",
    "            log_dict[f\"train_loss/{task_name}\"] = task_loss\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if learning_kwargs['use_cosine']:\n",
    "            scheduler.step()\n",
    "            log_dict[\"learning_rate\"] = scheduler.get_last_lr()[0]\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if eval_callback_fn is not None:\n",
    "            eval_metrics = eval_callback_fn(model, epoch=iter_idx)\n",
    "            test_losses.append(eval_metrics)\n",
    "            # Add eval metrics to wandb logging\n",
    "            if eval_metrics:  # Only log when we actually have eval metrics\n",
    "                for metric_name, value in eval_metrics.items():\n",
    "                    log_dict[f\"eval/{metric_name}\"] = value\n",
    "            print(test_losses[-1])\n",
    "        \n",
    "        # Log metrics to wandb\n",
    "        wandb.log(log_dict, step=iter_idx+1)\n",
    "\n",
    "    if len(test_losses) > 0:\n",
    "        return train_losses, test_losses\n",
    "    return train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2fc4f9265a4791ad71ea5b0d282494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1364fc3064d4fa893efcbe90ddb288a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/71 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd9c397ef43417695fbe1dd347440ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b036634f13456d90b1c6ec6f647595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test dataset available. Using train dataset for testing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_iters:  35 num_test_iters:  35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilliphguo\u001b[0m (\u001b[33mquirky_lats_at_mats\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/sae-editing/wandb/run-20250105_104527-eqo7m1sk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/quirky_lats_at_mats/sae-relearning/runs/eqo7m1sk' target=\"_blank\">pretty-snowflake-15</a></strong> to <a href='https://wandb.ai/quirky_lats_at_mats/sae-relearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/quirky_lats_at_mats/sae-relearning' target=\"_blank\">https://wandb.ai/quirky_lats_at_mats/sae-relearning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/quirky_lats_at_mats/sae-relearning/runs/eqo7m1sk' target=\"_blank\">https://wandb.ai/quirky_lats_at_mats/sae-relearning/runs/eqo7m1sk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_train_losses:  {'bio': 7.70703125, 'pile': 2.099609375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial test evaluations:  {'MMLU': 0.57, 'train_bio_acc': 0.2464634486607143, 'test_bio_acc': 0.2337681361607143}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52408d8a1274ecba50c5e6b8d8a7479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.6, 'train_bio_acc': 0.35147879464285714, 'test_bio_acc': 0.33214285714285713}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.64, 'train_bio_acc': 0.42765066964285714, 'test_bio_acc': 0.36612723214285714}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.5677455357142858, 'test_bio_acc': 0.4493861607142857}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.7338169642857143, 'test_bio_acc': 0.5405133928571428}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.80859375, 'test_bio_acc': 0.5583147321428571}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.8629464285714286, 'test_bio_acc': 0.587109375}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.8931919642857142, 'test_bio_acc': 0.5850446428571429}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.9248883928571429, 'test_bio_acc': 0.5720982142857143}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.9464285714285714, 'test_bio_acc': 0.5819196428571428}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.9553571428571429, 'test_bio_acc': 0.5838727678571428}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.67, 'train_bio_acc': 0.9690848214285714, 'test_bio_acc': 0.5938616071428572}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.67, 'train_bio_acc': 0.9735491071428571, 'test_bio_acc': 0.6041573660714286}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.67, 'train_bio_acc': 0.9704241071428571, 'test_bio_acc': 0.5964564732142857}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.67, 'train_bio_acc': 0.9797991071428571, 'test_bio_acc': 0.6064732142857143}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.67, 'train_bio_acc': 0.9819196428571428, 'test_bio_acc': 0.6006696428571429}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.67, 'train_bio_acc': 0.9783482142857143, 'test_bio_acc': 0.5950892857142858}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.9818080357142858, 'test_bio_acc': 0.6069196428571428}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.9849330357142857, 'test_bio_acc': 0.59921875}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.9830357142857142, 'test_bio_acc': 0.6078125}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.9822544642857143, 'test_bio_acc': 0.5976004464285715}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.9848214285714286, 'test_bio_acc': 0.5958705357142857}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.9802455357142857, 'test_bio_acc': 0.5997767857142857}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.9834821428571429, 'test_bio_acc': 0.5987723214285714}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.9866071428571429, 'test_bio_acc': 0.6047991071428571}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.66, 'train_bio_acc': 0.9814732142857143, 'test_bio_acc': 0.5989955357142858}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/MMLU</td><td>▁▃▆▇▇▇▇▇▇▇▇██████▇▇▇▇▇▇▇▇▇</td></tr><tr><td>eval/test_bio_acc</td><td>▁▃▃▅▇▇██▇█████████████████</td></tr><tr><td>eval/train_bio_acc</td><td>▁▂▃▄▆▆▇▇▇█████████████████</td></tr><tr><td>learning_rate</td><td>████████▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_loss/bio</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss/pile</td><td>▂▂▅▃▂▁▅▃▄▆▄▃▅▁▄▅▅▇▆▄▆▄▃▄▆▇█▅▄▆▆▂▇▆▅▇▇▅▃▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/MMLU</td><td>0.66</td></tr><tr><td>eval/test_bio_acc</td><td>0.599</td></tr><tr><td>eval/train_bio_acc</td><td>0.98147</td></tr><tr><td>learning_rate</td><td>0</td></tr><tr><td>train_loss/bio</td><td>0.03719</td></tr><tr><td>train_loss/pile</td><td>2.18103</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pretty-snowflake-15</strong> at: <a href='https://wandb.ai/quirky_lats_at_mats/sae-relearning/runs/eqo7m1sk' target=\"_blank\">https://wandb.ai/quirky_lats_at_mats/sae-relearning/runs/eqo7m1sk</a><br> View project at: <a href='https://wandb.ai/quirky_lats_at_mats/sae-relearning' target=\"_blank\">https://wandb.ai/quirky_lats_at_mats/sae-relearning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250105_104527-eqo7m1sk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tasks.general.DatasetTasks import PileTask\n",
    "train_batch_size = 2\n",
    "train_bio_task = WMDP_DedupedTask(batch_size=train_batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True)\n",
    "train_pile_task = PileTask(batch_size=train_batch_size, tokenizer=tokenizer, stream_dataset=True, buffer_size=10000, ctx_length=100)\n",
    "train_tasks = {\"bio\": (train_bio_task, .1), \"pile\": (train_pile_task, 1)}\n",
    "relearning_regular_results = {}\n",
    "n_relearn_iters = 100\n",
    "model.cuda()\n",
    "\n",
    "eval_batch_size = 8\n",
    "eval_bio_task = WMDP_DedupedTask(batch_size=eval_batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True)\n",
    "num_train_iters = len(eval_bio_task.train_dataset) // eval_batch_size\n",
    "num_test_iters = (len(eval_bio_task.test_dataset) * 4) // eval_batch_size\n",
    "\n",
    "print(\"num_train_iters: \", num_train_iters, \"num_test_iters: \", num_test_iters)\n",
    "\n",
    "evaluate_every = 4\n",
    "grad_accum_steps = 16\n",
    "\n",
    "def eval_callback(model, epoch, evaluate_every=evaluate_every):\n",
    "    if (epoch+1) % evaluate_every == 0:\n",
    "        mmlu_score = run_general_evals(model, model_type=model_type, evals_to_include=[\"MMLU\"], verbose=False, batch_size=2, device=\"cuda\")[\"MMLU\"]\n",
    "        train_bio_acc = eval_bio_task.get_test_accuracy(model, use_test_data=False, num_iters=num_train_iters)\n",
    "        test_bio_acc = eval_bio_task.get_test_accuracy(model, use_test_data=True, num_iters=num_test_iters)\n",
    "        return {\"MMLU\": mmlu_score, \"train_bio_acc\": train_bio_acc, \"test_bio_acc\": test_bio_acc}\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "if is_lora:\n",
    "    lr = 5e-6\n",
    "else:\n",
    "    lr = 5e-6\n",
    "finetune_lora = False\n",
    "wandb.init(\n",
    "    project=\"sae-relearning\",\n",
    "    config={\n",
    "        \"model_name\": model_name_or_path,\n",
    "        \"pretrained_path\": pretrained_path,\n",
    "        \"lr\": lr,\n",
    "        \"finetune_lora\": finetune_lora,\n",
    "        \"n_iterations\": n_relearn_iters,\n",
    "        \"grad_accum_steps\": grad_accum_steps\n",
    "    }\n",
    ")\n",
    "\n",
    "init_log_dict = {}\n",
    "with torch.no_grad():\n",
    "    initial_train_losses = {}\n",
    "    for task_name, (task, task_weight) in train_tasks.items():\n",
    "        task_loss = 0\n",
    "        for i in range(grad_accum_steps):\n",
    "            loss = task.get_train_loss(model) / grad_accum_steps\n",
    "            task_loss += loss.item()\n",
    "        initial_train_losses[task_name] = task_loss\n",
    "        init_log_dict[f\"train_loss/{task_name}\"] = task_loss\n",
    "print(\"initial_train_losses: \", initial_train_losses)\n",
    "initial_test_loss = eval_callback(model, epoch=-1)\n",
    "for metric_name, value in initial_test_loss.items():\n",
    "    init_log_dict[f\"eval/{metric_name}\"] = value\n",
    "print(\"Initial test evaluations: \", initial_test_loss)\n",
    "\n",
    "wandb.log(init_log_dict, step=0)\n",
    "\n",
    "train_losses, test_losses = do_relearning(model, train_tasks, n_iters=n_relearn_iters, grad_accum_steps=grad_accum_steps, finetune_lora=finetune_lora, learning_kwargs={'lr': lr, 'weight_decay': 0, 'use_cosine': True}, eval_callback_fn=eval_callback)\n",
    "\n",
    "test_losses.insert(0, initial_test_loss)\n",
    "\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
