{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/sae-editing\n",
      "Successfully authenticated with Hugging Face.\n",
      "Successfully authenticated with Weights & Biases.\n"
     ]
    }
   ],
   "source": [
    "# from circuit_breaking.src import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import contextlib\n",
    "import einops\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import bitsandbytes as bnb\n",
    "from collections import defaultdict\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "HF_ACCESS_TOKEN = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "if HF_ACCESS_TOKEN:\n",
    "    login(token=HF_ACCESS_TOKEN)\n",
    "    print(\"Successfully authenticated with Hugging Face.\")\n",
    "else:\n",
    "    print(\"Hugging Face access token not found in environment variables.\")\n",
    "\n",
    "if WANDB_API_KEY:\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "    print(\"Successfully authenticated with Weights & Biases.\")\n",
    "else:\n",
    "    print(\"Weights & Biases API key not found in environment variables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3caf4d93c7844b73a477fd57d551cabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = \"google/gemma-2-9b\"\n",
    "model_type = \"gemma-2\"\n",
    "other_model_type = \"gemma2_9b\"\n",
    "# pretrained_path = \"/data/huggingface/models--google--gemma-2-9b/snapshots/33c193028431c2fde6c6e51f29e6f17b60cbfac6/\"\n",
    "# pretrained_path = \"/data/huggingface/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/\"\n",
    "# pretrained_path = \"gemma-2-rmu-fullrank\"\n",
    "pretrained_path = \"gemma-2-rmu-fullrank\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "left_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "left_tokenizer.pad_token_id = left_tokenizer.eos_token_id\n",
    "left_tokenizer.padding_side = \"left\"\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "if pretrained_path is not None:\n",
    "    model = AutoModelForCausalLM.from_pretrained(pretrained_path, torch_dtype=dtype)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype)\n",
    "model.cuda()\n",
    "n_layers = 42\n",
    "n_heads = 16\n",
    "n_kv_heads = 8\n",
    "\n",
    "param_count_dict = {\"attn.hook_q\": 3584*4096, \"attn.hook_k\": 3584*2048, \"attn.hook_v\": 3584*2048, \"attn.hook_result\": 4096*3584, \"mlp.hook_pre\": 3584 * 14336, \"mlp.hook_post\": 14336 * 3584, \"mlp.hook_gate\": 3584 * 14336}\n",
    "mmlu_batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63604ff9be64ee7ab8e81177f328f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1273 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tasks.wmdp.WMDP_MCTask import WMDP_MCTask, WMDP_DedupedTask\n",
    "from tasks.wmdp.WMDP_RelearnTask import WMDP_RelearnTask\n",
    "from tasks.general_capabilities.MCTask_redo import run_general_evals\n",
    "batch_size = 8\n",
    "bio_mc_task = WMDP_MCTask(batch_size=batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True)\n",
    "num_iters = len(bio_mc_task.dataset) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_logits.argmax(dim=1)=tensor([608, 599, 585, 585, 585, 599, 608, 608], device='cuda:0'), tokenized_labels=tensor([586, 599, 586, 585, 585, 599, 608, 586], device='cuda:0'), probs[range(len(last_logits)), tokenized_labels]=tensor([0.1523, 0.6406, 0.1709, 0.7656, 0.4414, 0.8359, 0.8203, 0.1143],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "last_logits.argmax(dim=1)=tensor([586, 586, 599, 608, 608, 599, 599, 599], device='cuda:0'), tokenized_labels=tensor([585, 586, 586, 608, 608, 599, 599, 599], device='cuda:0'), probs[range(len(last_logits)), tokenized_labels]=tensor([0.2168, 0.8828, 0.3203, 0.8164, 0.6836, 0.6836, 0.6250, 0.6992],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "last_logits.argmax(dim=1)=tensor([586, 586, 585, 599, 586, 585, 586, 585], device='cuda:0'), tokenized_labels=tensor([586, 586, 585, 585, 585, 586, 586, 585], device='cuda:0'), probs[range(len(last_logits)), tokenized_labels]=tensor([0.4668, 0.6992, 0.5508, 0.0869, 0.0386, 0.1416, 0.2656, 0.6562],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "last_logits.argmax(dim=1)=tensor([586, 586, 608, 585, 608, 608, 585, 586], device='cuda:0'), tokenized_labels=tensor([608, 585, 599, 585, 599, 586, 585, 586], device='cuda:0'), probs[range(len(last_logits)), tokenized_labels]=tensor([0.0601, 0.1465, 0.0732, 0.3613, 0.1455, 0.2021, 0.8203, 0.5391],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "last_logits.argmax(dim=1)=tensor([585, 585, 599, 586, 585, 586, 586, 586], device='cuda:0'), tokenized_labels=tensor([585, 599, 585, 599, 585, 586, 586, 586], device='cuda:0'), probs[range(len(last_logits)), tokenized_labels]=tensor([0.8398, 0.2119, 0.1475, 0.1943, 0.3789, 0.3770, 0.8281, 0.6016],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(1.1094, device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "bio_mc_deduped_task = WMDP_DedupedTask(batch_size=batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True)\n",
    "num_iters_deduped = len(bio_mc_deduped_task.train_dataset) // batch_size\n",
    "\n",
    "bio_mc_deduped_task.get_test_accuracy(model, use_test_data=False, num_iters=num_iters_deduped)\n",
    "print(bio_mc_deduped_task.get_test_loss(model, n_iters=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bio dataset is size  71 and cyber dataset is size  79 Missed  7 data points in train split  0\n",
      "Bio dataset is size  71 and cyber dataset is size  76 Missed  10 data points in train split  1\n",
      "Bio dataset is size  71 and cyber dataset is size  84 Missed  2 data points in train split  2\n",
      "Bio dataset is size  71 and cyber dataset is size  79 Missed  7 data points in train split  3\n",
      "Bio dataset is size  71 and cyber dataset is size  75 Missed  11 data points in test split  4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef86fd14be148839ce969787a385aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ce302b4d0e4c22a33651e36d8c671d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c712011b869c4a919f4f6198ff9f123c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3184a3f40d514798b781173a860c4717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972a9ac6507a4e588180f141eabcfc65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97144514ee2e424b91efd9d01e0ac70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a30c7d5b686434c9a29e2b970a9cf6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ad80895c9049d4b75e8f5fe3903041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb5a11cd69b47d68b3ce609620c9d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/PhillipGuo/wmdp-deduped/commit/6247bd97fef16a2024b12c874b2badaa49bae202', commit_message='Upload dataset', commit_description='', oid='6247bd97fef16a2024b12c874b2badaa49bae202', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/PhillipGuo/wmdp-deduped', endpoint='https://huggingface.co', repo_type='dataset', repo_id='PhillipGuo/wmdp-deduped'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# original_bio_df = load_dataset(\"cais/wmdp\", \"wmdp-bio\", split=\"test\").to_pandas()\n",
    "# original_cyber_df = load_dataset(\"cais/wmdp\", \"wmdp-cyber\", split=\"test\").to_pandas()\n",
    "\n",
    "# # load in deduped data, split into train and test\n",
    "# bio_train_dfs = []\n",
    "# cyber_train_dfs = []\n",
    "# for train_split_idx in [0, 1, 2, 3]:\n",
    "#     # tasks/wmdp/data/mcq_split_0.jsonl\n",
    "#     mcq_formatted_data = pd.read_json(f\"tasks/wmdp/data/mcq_split_{train_split_idx}.jsonl\", lines=True)\n",
    "#     bio_indices = mcq_formatted_data[\"question\"].isin(original_bio_df[\"question\"])\n",
    "#     cyber_indices = mcq_formatted_data[\"question\"].isin(original_cyber_df[\"question\"])\n",
    "#     # display(bio_indices + cyber_indices)\n",
    "#     # display(mcq_formatted_data[~(bio_indices + cyber_indices)])\n",
    "#     print(\"Bio dataset is size \", len(mcq_formatted_data[bio_indices]), \"and cyber dataset is size \", len(mcq_formatted_data[cyber_indices]), \"Missed \", len(mcq_formatted_data[~(bio_indices + cyber_indices)]), \"data points in train split \", train_split_idx)\n",
    "#     bio_train_dfs.append(mcq_formatted_data[bio_indices])\n",
    "#     cyber_train_dfs.append(mcq_formatted_data[cyber_indices])\n",
    "\n",
    "# bio_test_dfs = []\n",
    "# cyber_test_dfs = []\n",
    "# for test_split_idx in [4]:\n",
    "#     mcq_formatted_data = pd.read_json(f\"tasks/wmdp/data/mcq_split_{test_split_idx}.jsonl\", lines=True)\n",
    "#     bio_indices = mcq_formatted_data[\"question\"].isin(original_bio_df[\"question\"])\n",
    "#     cyber_indices = mcq_formatted_data[\"question\"].isin(original_cyber_df[\"question\"])\n",
    "#     bio_test_dfs.append(mcq_formatted_data[bio_indices])\n",
    "#     cyber_test_dfs.append(mcq_formatted_data[cyber_indices])\n",
    "#     print(\"Bio dataset is size \", len(mcq_formatted_data[bio_indices]), \"and cyber dataset is size \", len(mcq_formatted_data[cyber_indices]), \"Missed \", len(mcq_formatted_data[~(bio_indices + cyber_indices)]), \"data points in test split \", test_split_idx)\n",
    "\n",
    "# bio_train_df = pd.concat(bio_train_dfs, ignore_index=True)\n",
    "# cyber_train_df = pd.concat(cyber_train_dfs, ignore_index=True)\n",
    "# bio_test_df = pd.concat(bio_test_dfs, ignore_index=True)\n",
    "# cyber_test_df = pd.concat(cyber_test_dfs, ignore_index=True)\n",
    "# from datasets import Dataset, DatasetDict\n",
    "# # convert to huggingface dataset\n",
    "# bio_train_dataset = Dataset.from_pandas(bio_train_df)\n",
    "# cyber_train_dataset = Dataset.from_pandas(cyber_train_df)\n",
    "# bio_test_dataset = Dataset.from_pandas(bio_test_df)\n",
    "# cyber_test_dataset = Dataset.from_pandas(cyber_test_df)\n",
    "\n",
    "# # Create separate DatasetDicts for bio and cyber\n",
    "# bio_dataset = DatasetDict({\n",
    "#     \"train\": bio_train_dataset,\n",
    "#     \"test\": bio_test_dataset\n",
    "# })\n",
    "\n",
    "# cyber_dataset = DatasetDict({\n",
    "#     \"train\": cyber_train_dataset,\n",
    "#     \"test\": cyber_test_dataset\n",
    "# })\n",
    "# bio_dataset.push_to_hub(\"PhillipGuo/wmdp-deduped\", \"wmdp-bio\")\n",
    "# cyber_dataset.push_to_hub(\"PhillipGuo/wmdp-deduped\", \"wmdp-cyber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_callback(model, epoch, evaluate_every=10):\n",
    "    if (epoch+1) % evaluate_every == 0:\n",
    "        mmlu_score = run_general_evals(model, model_type=model_type, evals_to_include=[\"MMLU\"], verbose=False, batch_size=2, device=\"cuda\")[\"MMLU\"]\n",
    "        bio_acc = bio_mc_task.get_test_accuracy(model, num_iters=num_iters)\n",
    "        return {\"MMLU\": mmlu_score, \"bio_acc\": bio_acc}\n",
    "    else:\n",
    "        return {}\n",
    "    \n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "def do_relearning(model, train_tasks, n_iters, grad_accum_steps=1, finetune_lora=False, lora_kwargs={'rank': 256, 'alpha': 32, 'dropout': 0.05, 'target_modules': 'all-linear'}, learning_kwargs={'lr': 1e-5, 'weight_decay': 0, 'use_cosine': False}, eval_callback_fn=None, forget_kwargs=None, maintain_kwargs=None, inject_label=None):\n",
    "    # can either finetune full or lora\n",
    "\n",
    "    if not finetune_lora:\n",
    "        optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=learning_kwargs['lr'], weight_decay=learning_kwargs['weight_decay'])\n",
    "\n",
    "    elif finetune_lora:\n",
    "        peft_config = LoraConfig(\n",
    "            inference_mode=False,\n",
    "            r=lora_kwargs['rank'],\n",
    "            lora_alpha=lora_kwargs['alpha'],\n",
    "            lora_dropout=lora_kwargs['dropout'],\n",
    "            target_modules = lora_kwargs['target_modules'], #[\"q_proj\", \"v_proj\", \n",
    "        )\n",
    "\n",
    "        model = get_peft_model(model, peft_config).cuda()\n",
    "        # model.print_trainable_parameters()\n",
    "        print(f\"Parameters in peft: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_kwargs['lr'], weight_decay=learning_kwargs['weight_decay'])\n",
    "    \n",
    "    if learning_kwargs['use_cosine']:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=n_iters)\n",
    "\n",
    "    train_losses = defaultdict(list)\n",
    "    test_losses = []\n",
    "\n",
    "    for iter_idx in tqdm(range(n_iters)):\n",
    "        optimizer.zero_grad()\n",
    "        for task_name, (task, task_weight) in train_tasks.items():\n",
    "            task_loss = 0\n",
    "            for i in range(grad_accum_steps):\n",
    "                loss = task.get_train_loss(model) / grad_accum_steps\n",
    "                task_loss += loss.item()\n",
    "                # print(loss.item())\n",
    "                (loss * task_weight).backward()\n",
    "            train_losses[task_name].append(task_loss)\n",
    "            print(f\"{task_name} loss: {task_loss}\")\n",
    "            print(f\"Memory after {task_name} loss: {torch.cuda.memory_allocated() / 10**9} GB\")\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if learning_kwargs['use_cosine']:\n",
    "            scheduler.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Memory after optimizer step: {torch.cuda.memory_allocated() / 10**9} GB\")\n",
    "\n",
    "        if eval_callback_fn is not None:\n",
    "            test_losses.append(eval_callback_fn(model, epoch=iter_idx, forget_kwargs=forget_kwargs, maintain_kwargs=maintain_kwargs, inject_label=inject_label))\n",
    "            print(test_losses[-1])\n",
    "\n",
    "    if len(test_losses) > 0:\n",
    "        return train_losses, test_losses\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "# del model\n",
    "\n",
    "# for name, model, mask, regular_evals, side_effect_evals, adversarial_evals in [(\"localized\", localized_model, localized_mask, localized_regular_evals, localized_side_effect_evals, localized_adversarial_evals), (\"nonlocalized\", nonlocalized_model, nonlocalized_mask, nonlocalized_regular_evals, nonlocalized_side_effect_evals, nonlocalized_adversarial_evals)]:\n",
    "\n",
    "relearning_regular_results = {}\n",
    "# relearning_adversarial_results = {}\n",
    "# relearning_side_effect_results = {}\n",
    "\n",
    "model.cuda()\n",
    "initial_test_loss = eval_callback(model, epoch=-1, forget_kwargs=relearn_forget_kwargs, maintain_kwargs=relearn_maintain_kwargs, inject_label=inject_label)\n",
    "\n",
    "print(torch.cuda.memory_allocated() / 10**9, \"GB\")\n",
    "print(train_tasks)\n",
    "train_losses, test_losses = do_relearning(model, train_tasks, n_iters=n_relearn_iters, finetune_lora=True, lora_kwargs={'rank': 512, 'alpha': 32, 'dropout': 0.05, 'target_modules': 'all-linear'}, learning_kwargs={'lr': 2e-4, 'weight_decay': 0, 'use_cosine': True}, eval_callback_fn=eval_callback, forget_kwargs=relearn_forget_kwargs, maintain_kwargs=relearn_maintain_kwargs, inject_label=inject_label)\n",
    "\n",
    "test_losses.insert(0, initial_test_loss)\n",
    "\n",
    "for task_name, test_task in [(\"forget_sport\", forget_sport_eval), (\"maintain_sports\", maintain_sports_eval)]:\n",
    "    task_loss = 0\n",
    "    task_accuracy = 0\n",
    "    for i in range(n_eval_iters):\n",
    "        task_loss += test_task.get_test_loss(model).item()\n",
    "        task_accuracy += test_task.get_test_accuracy(model)\n",
    "    relearning_regular_results[f\"{task_name}_ce\"] = task_loss / n_eval_iters\n",
    "    relearning_regular_results[f\"{task_name}_acc\"] = task_accuracy / n_eval_iters\n",
    "# model.cpu()\n",
    "\n",
    "os.makedirs(f\"{save_dir}/results\", exist_ok=True)\n",
    "with open(f\"{save_dir}/results/relearning_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"relearning_regular_results\": relearning_regular_results, \"relearning_train_losses\": train_losses, \"relearning_test_losses\": test_losses}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
