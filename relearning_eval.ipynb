{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/sae-editing\n",
      "Successfully authenticated with Hugging Face.\n",
      "Successfully authenticated with Weights & Biases.\n"
     ]
    }
   ],
   "source": [
    "# from circuit_breaking.src import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import contextlib\n",
    "import einops\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import bitsandbytes as bnb\n",
    "from collections import defaultdict\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "HF_ACCESS_TOKEN = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "if HF_ACCESS_TOKEN:\n",
    "    login(token=HF_ACCESS_TOKEN)\n",
    "    print(\"Successfully authenticated with Hugging Face.\")\n",
    "else:\n",
    "    print(\"Hugging Face access token not found in environment variables.\")\n",
    "\n",
    "if WANDB_API_KEY:\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "    print(\"Successfully authenticated with Weights & Biases.\")\n",
    "else:\n",
    "    print(\"Weights & Biases API key not found in environment variables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db486cbc7684442a3d29885ef85ccf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = \"google/gemma-2-9b\"\n",
    "model_type = \"gemma-2\"\n",
    "other_model_type = \"gemma2_9b\"\n",
    "# pretrained_path = \"/data/huggingface/models--google--gemma-2-9b/snapshots/33c193028431c2fde6c6e51f29e6f17b60cbfac6/\"\n",
    "# pretrained_path = \"/data/huggingface/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/\"\n",
    "# pretrained_path = \"gemma-2-rmu-fullrank\"\n",
    "pretrained_path = \"gemma-2-rmu-fullrank\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "left_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "left_tokenizer.pad_token_id = left_tokenizer.eos_token_id\n",
    "left_tokenizer.padding_side = \"left\"\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "if pretrained_path is not None:\n",
    "    model = AutoModelForCausalLM.from_pretrained(pretrained_path, torch_dtype=dtype)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype)\n",
    "model.cuda()\n",
    "n_layers = 42\n",
    "n_heads = 16\n",
    "n_kv_heads = 8\n",
    "\n",
    "param_count_dict = {\"attn.hook_q\": 3584*4096, \"attn.hook_k\": 3584*2048, \"attn.hook_v\": 3584*2048, \"attn.hook_result\": 4096*3584, \"mlp.hook_pre\": 3584 * 14336, \"mlp.hook_post\": 14336 * 3584, \"mlp.hook_gate\": 3584 * 14336}\n",
    "mmlu_batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.wmdp.WMDP_MCTask import WMDP_MCTask, WMDP_DedupedTask\n",
    "from tasks.wmdp.WMDP_RelearnTask import WMDP_RelearnTask\n",
    "from tasks.general_capabilities.MCTask_redo import run_general_evals\n",
    "batch_size = 8\n",
    "bio_mc_task = WMDP_MCTask(batch_size=batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True)\n",
    "num_iters = len(bio_mc_task.dataset) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46875\n",
      "tensor(1.6719, device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "bio_mc_deduped_task = WMDP_DedupedTask(batch_size=batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True)\n",
    "num_train_iters = len(bio_mc_deduped_task.train_dataset) // batch_size\n",
    "num_test_iters = len(bio_mc_deduped_task.test_dataset) // batch_size\n",
    "\n",
    "print(bio_mc_deduped_task.get_test_accuracy(model, use_test_data=True, num_iters=num_test_iters))\n",
    "print(bio_mc_deduped_task.get_test_loss(model, n_iters=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# original_bio_df = load_dataset(\"cais/wmdp\", \"wmdp-bio\", split=\"test\").to_pandas()\n",
    "# original_cyber_df = load_dataset(\"cais/wmdp\", \"wmdp-cyber\", split=\"test\").to_pandas()\n",
    "\n",
    "# # load in deduped data, split into train and test\n",
    "# bio_train_dfs = []\n",
    "# cyber_train_dfs = []\n",
    "# for train_split_idx in [0, 1, 2, 3]:\n",
    "#     # tasks/wmdp/data/mcq_split_0.jsonl\n",
    "#     mcq_formatted_data = pd.read_json(f\"tasks/wmdp/data/mcq_split_{train_split_idx}.jsonl\", lines=True)\n",
    "#     bio_indices = mcq_formatted_data[\"question\"].isin(original_bio_df[\"question\"])\n",
    "#     cyber_indices = mcq_formatted_data[\"question\"].isin(original_cyber_df[\"question\"])\n",
    "#     # display(bio_indices + cyber_indices)\n",
    "#     # display(mcq_formatted_data[~(bio_indices + cyber_indices)])\n",
    "#     print(\"Bio dataset is size \", len(mcq_formatted_data[bio_indices]), \"and cyber dataset is size \", len(mcq_formatted_data[cyber_indices]), \"Missed \", len(mcq_formatted_data[~(bio_indices + cyber_indices)]), \"data points in train split \", train_split_idx)\n",
    "#     bio_train_dfs.append(mcq_formatted_data[bio_indices])\n",
    "#     cyber_train_dfs.append(mcq_formatted_data[cyber_indices])\n",
    "\n",
    "# bio_test_dfs = []\n",
    "# cyber_test_dfs = []\n",
    "# for test_split_idx in [4]:\n",
    "#     mcq_formatted_data = pd.read_json(f\"tasks/wmdp/data/mcq_split_{test_split_idx}.jsonl\", lines=True)\n",
    "#     bio_indices = mcq_formatted_data[\"question\"].isin(original_bio_df[\"question\"])\n",
    "#     cyber_indices = mcq_formatted_data[\"question\"].isin(original_cyber_df[\"question\"])\n",
    "#     bio_test_dfs.append(mcq_formatted_data[bio_indices])\n",
    "#     cyber_test_dfs.append(mcq_formatted_data[cyber_indices])\n",
    "#     print(\"Bio dataset is size \", len(mcq_formatted_data[bio_indices]), \"and cyber dataset is size \", len(mcq_formatted_data[cyber_indices]), \"Missed \", len(mcq_formatted_data[~(bio_indices + cyber_indices)]), \"data points in test split \", test_split_idx)\n",
    "\n",
    "# bio_train_df = pd.concat(bio_train_dfs, ignore_index=True)\n",
    "# cyber_train_df = pd.concat(cyber_train_dfs, ignore_index=True)\n",
    "# bio_test_df = pd.concat(bio_test_dfs, ignore_index=True)\n",
    "# cyber_test_df = pd.concat(cyber_test_dfs, ignore_index=True)\n",
    "# from datasets import Dataset, DatasetDict\n",
    "# # convert to huggingface dataset\n",
    "# bio_train_dataset = Dataset.from_pandas(bio_train_df)\n",
    "# cyber_train_dataset = Dataset.from_pandas(cyber_train_df)\n",
    "# bio_test_dataset = Dataset.from_pandas(bio_test_df)\n",
    "# cyber_test_dataset = Dataset.from_pandas(cyber_test_df)\n",
    "\n",
    "# # Create separate DatasetDicts for bio and cyber\n",
    "# bio_dataset = DatasetDict({\n",
    "#     \"train\": bio_train_dataset,\n",
    "#     \"test\": bio_test_dataset\n",
    "# })\n",
    "\n",
    "# cyber_dataset = DatasetDict({\n",
    "#     \"train\": cyber_train_dataset,\n",
    "#     \"test\": cyber_test_dataset\n",
    "# })\n",
    "# bio_dataset.push_to_hub(\"PhillipGuo/wmdp-deduped\", \"wmdp-bio\")\n",
    "# cyber_dataset.push_to_hub(\"PhillipGuo/wmdp-deduped\", \"wmdp-cyber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_callback(model, epoch, evaluate_every=10):\n",
    "    if (epoch+1) % evaluate_every == 0:\n",
    "        mmlu_score = run_general_evals(model, model_type=model_type, evals_to_include=[\"MMLU\"], verbose=False, batch_size=2, device=\"cuda\")[\"MMLU\"]\n",
    "        train_bio_acc = bio_mc_task.get_test_accuracy(model, use_test_data=False, num_iters=num_train_iters)\n",
    "        test_bio_acc = bio_mc_task.get_test_accuracy(model, use_test_data=True, num_iters=num_test_iters)\n",
    "        return {\"MMLU\": mmlu_score, \"train_bio_acc\": train_bio_acc, \"test_bio_acc\": test_bio_acc}\n",
    "    else:\n",
    "        return {}\n",
    "    \n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "def do_relearning(model, train_tasks, n_iters, grad_accum_steps=1, finetune_lora=False, lora_kwargs={'rank': 256, 'alpha': 32, 'dropout': 0.05, 'target_modules': 'all-linear'}, learning_kwargs={'lr': 1e-5, 'weight_decay': 0, 'use_cosine': False}, eval_callback_fn=None):\n",
    "    # can either finetune full or lora\n",
    "\n",
    "    if not finetune_lora:\n",
    "        optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=learning_kwargs['lr'], weight_decay=learning_kwargs['weight_decay'])\n",
    "\n",
    "    elif finetune_lora:\n",
    "        peft_config = LoraConfig(\n",
    "            inference_mode=False,\n",
    "            r=lora_kwargs['rank'],\n",
    "            lora_alpha=lora_kwargs['alpha'],\n",
    "            lora_dropout=lora_kwargs['dropout'],\n",
    "            target_modules = lora_kwargs['target_modules'], #[\"q_proj\", \"v_proj\", \n",
    "        )\n",
    "\n",
    "        model = get_peft_model(model, peft_config).cuda()\n",
    "        # model.print_trainable_parameters()\n",
    "        print(f\"Parameters in peft: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_kwargs['lr'], weight_decay=learning_kwargs['weight_decay'])\n",
    "    \n",
    "    if learning_kwargs['use_cosine']:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=n_iters)\n",
    "\n",
    "    train_losses = defaultdict(list)\n",
    "    test_losses = []\n",
    "\n",
    "    for iter_idx in tqdm(range(n_iters)):\n",
    "        optimizer.zero_grad()\n",
    "        for task_name, (task, task_weight) in train_tasks.items():\n",
    "            task_loss = 0\n",
    "            for i in range(grad_accum_steps):\n",
    "                loss = task.get_train_loss(model) / grad_accum_steps\n",
    "                task_loss += loss.item()\n",
    "                (loss * task_weight).backward()\n",
    "            train_losses[task_name].append(task_loss)\n",
    "            # print(f\"{task_name} loss: {task_loss}\")\n",
    "            # print(f\"Memory after {task_name} loss: {torch.cuda.memory_allocated() / 10**9} GB\")\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if learning_kwargs['use_cosine']:\n",
    "            scheduler.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        # print(f\"Memory after optimizer step: {torch.cuda.memory_allocated() / 10**9} GB\")\n",
    "\n",
    "        if eval_callback_fn is not None:\n",
    "            test_losses.append(eval_callback_fn(model, epoch=iter_idx))\n",
    "            print(test_losses[-1])\n",
    "\n",
    "    if len(test_losses) > 0:\n",
    "        return train_losses, test_losses\n",
    "    return train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a5602cc80341aa9a552d8dfe004307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa729837e0ea44cc8410a6c4043d5108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test dataset available. Using train dataset for testing.\n",
      "initial_train_losses:  {'bio': 1.8994140625, 'pile': 2.6533203125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before relearning, eval callback results:  {'MMLU': 0.67, 'train_bio_acc': 0.42857142857142855, 'test_bio_acc': 0.34375}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce50e26a4314f0bb7ba03afb732ce17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.7, 'train_bio_acc': 0.48928571428571427, 'test_bio_acc': 0.5}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.7, 'train_bio_acc': 0.4785714285714286, 'test_bio_acc': 0.5}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.68, 'train_bio_acc': 0.4714285714285714, 'test_bio_acc': 0.484375}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.67, 'train_bio_acc': 0.5107142857142857, 'test_bio_acc': 0.46875}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.68, 'train_bio_acc': 0.5071428571428571, 'test_bio_acc': 0.453125}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.69, 'train_bio_acc': 0.4785714285714286, 'test_bio_acc': 0.390625}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.68, 'train_bio_acc': 0.5071428571428571, 'test_bio_acc': 0.46875}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MMLU': 0.69, 'train_bio_acc': 0.5, 'test_bio_acc': 0.5}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from tasks.general.DatasetTasks import PileTask\n",
    "train_batch_size = 2\n",
    "train_bio_task = WMDP_DedupedTask(batch_size=train_batch_size, tokenizer=tokenizer, subset=\"wmdp-bio\", shuffle=True)\n",
    "train_pile_task = PileTask(batch_size=train_batch_size, tokenizer=tokenizer, stream_dataset=True, buffer_size=10000, ctx_length=100)\n",
    "train_tasks = {\"bio\": (train_bio_task, .1), \"pile\": (train_pile_task, 1)}\n",
    "relearning_regular_results = {}\n",
    "n_relearn_iters = 100\n",
    "model.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    initial_train_losses = {}\n",
    "    for task_name, (task, task_weight) in train_tasks.items():\n",
    "        task_loss = 0\n",
    "        for i in range(10):\n",
    "            loss = task.get_train_loss(model) / 10\n",
    "            task_loss += loss.item()\n",
    "        initial_train_losses[task_name] = task_loss\n",
    "print(\"initial_train_losses: \", initial_train_losses)\n",
    "initial_test_loss = eval_callback(model, epoch=-1)\n",
    "print(\"Before relearning, eval callback results: \", initial_test_loss)\n",
    "\n",
    "train_losses, test_losses = do_relearning(model, train_tasks, n_iters=n_relearn_iters, finetune_lora=False, learning_kwargs={'lr': 1e-6, 'weight_decay': 0, 'use_cosine': True}, eval_callback_fn=eval_callback)\n",
    "\n",
    "test_losses.insert(0, initial_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
